{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41c3951",
   "metadata": {},
   "source": [
    "# Fine-Tuning Microsoft's Dayhoff Model for Antibody Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d877ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create --name dayhoff python=3.10\n",
    "# conda activate dayhoff\n",
    "# pip install dayhoff peft trl\n",
    "# conda install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c1fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To avoid \"device-side assert triggered\" RuntimeErrors\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "# os.environ[\"MAMBA_DISABLE_FAST_KERNELS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f125d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8aa9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config\n",
    "MODEL_ID = \"microsoft/Dayhoff-3b-UR90\"\n",
    "DATA_PATH = \"../data/sabdab/sabdab_training_dataset.csv\"  # CSV with columns: antigen, antibody\n",
    "OUTPUT_DIR = \"models/peleke-dayhoff-3b-ur90_2.01\"\n",
    "#CACHE_DIR = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/colby-h100-01-ci/code/.cache/huggingface/\"\n",
    "\n",
    "MAX_LEN = 3072 # old value2048\n",
    "BATCH_SIZE = 1 # old value 2\n",
    "GRAD_ACCUM = 8 # old value 8\n",
    "EPOCHS = 3\n",
    "LR = 5e-5 # old value 2e-4\n",
    "WARMUP = 0.03\n",
    "USE_QLORA = True\n",
    "SEED = 1337\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa79532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdee4e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens: {'bos_token': '@', 'eos_token': '*', 'sep_token': '/', 'pad_token': '!', 'mask_token': '#'}\n"
     ]
    }
   ],
   "source": [
    "## Tokenizer Setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    #cache_dir=CACHE_DIR\n",
    "    )\n",
    "\n",
    "## Add epitope tokens\n",
    "# epitope_tokens = [\"[\", \"]\"]\n",
    "# tokenizer.add_special_tokens({\"additional_special_tokens\": epitope_tokens})\n",
    "# special_tokens_dict = {\"additional_special_tokens\": ['|']}\n",
    "# num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "## Add amino acid tokens (which should all exist) and delimiter\n",
    "# amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "# extra_tokens = amino_acids + [\"|\"]\n",
    "# new_tokens = [t for t in extra_tokens if t not in tokenizer.get_vocab()]\n",
    "# tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# num_added = tokenizer.add_tokens(['|'])\n",
    "\n",
    "\n",
    "bnb_config = None\n",
    "# if USE_QLORA:\n",
    "#     from transformers import BitsAndBytesConfig\n",
    "#     bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#         bnb_4bit_use_double_quant=True,\n",
    "#         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     )\n",
    "\n",
    "## Confirm existing special tokens\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n",
    "# print(\"Num tokens added:\", num_added)\n",
    "# print(\"New tokenizer size:\", len(tokenizer))\n",
    "# print(\"ID for |:\", tokenizer.convert_tokens_to_ids(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dc49310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "## Model Setup\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    torch_dtype=\"auto\",\n",
    "    # device_map=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    use_mamba_kernels=False,\n",
    "    #cache_dir=CACHE_DIR\n",
    ")#.cuda()\n",
    "\n",
    "## Resize model to accomodate new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if USE_QLORA:\n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        target_modules=\"all-linear\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)#.cuda()\n",
    "\n",
    "#model.to(DEVICE)\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f575e301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2269 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SEQUENCE LENGTH ANALYSIS ===\n",
      "Sequence Length Analysis:\n",
      "Total sequences: 9523\n",
      "Mean length: 631.4\n",
      "Median length: 542.0\n",
      "Max length: 3560\n",
      "Min length: 224\n",
      "At max_length=1024: 1409/9523 truncated (14.8%) | 85.2% data retained\n",
      "At max_length=1536: 92/9523 truncated (1.0%) | 99.0% data retained\n",
      "At max_length=2048: 41/9523 truncated (0.4%) | 99.6% data retained\n",
      "At max_length=2560: 3/9523 truncated (0.0%) | 100.0% data retained\n",
      "At max_length=3072: 3/9523 truncated (0.0%) | 100.0% data retained\n"
     ]
    }
   ],
   "source": [
    "## Load Dataset (your current code)\n",
    "dataset = load_dataset(\"csv\", data_files=DATA_PATH)\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "columns_to_remove = [col for col in all_columns if col not in ['antibody_fv_seqs', 'highlighted_epitope_seqs']]\n",
    "dataset = dataset.remove_columns(columns_to_remove) \\\n",
    "    .rename_column(\"antibody_fv_seqs\", \"antibody\") \\\n",
    "    .rename_column(\"highlighted_epitope_seqs\", \"antigen\")\n",
    "\n",
    "## Pre-tokenization: Check sequence lengths BEFORE truncation\n",
    "def analyze_sequence_lengths(dataset, max_len=2048):\n",
    "    \"\"\"Analyze sequence lengths to determine optimal max_length\"\"\"\n",
    "    sequence_lengths = []\n",
    "    \n",
    "    for example in dataset:\n",
    "        # Build the text format (same as your build_example function)\n",
    "        allowed_chars = \"ACDEFGHIKLMNPQRSTVWY[]@*/#\"\n",
    "        antigen = \"\".join([c for c in example['antigen'] if c in allowed_chars])\n",
    "        antibody = \"\".join([c for c in example['antibody'].replace('|','/') if c in allowed_chars])\n",
    "        text = f\"@{antigen}/{antibody}*\"\n",
    "        \n",
    "        # Tokenize without truncation to get true length\n",
    "        tokens = tokenizer(text, add_special_tokens=False, truncation=False)\n",
    "        sequence_lengths.append(len(tokens[\"input_ids\"]))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    import numpy as np\n",
    "    lengths = np.array(sequence_lengths)\n",
    "    \n",
    "    print(f\"Sequence Length Analysis:\")\n",
    "    print(f\"Total sequences: {len(lengths)}\")\n",
    "    print(f\"Mean length: {np.mean(lengths):.1f}\")\n",
    "    print(f\"Median length: {np.median(lengths):.1f}\")\n",
    "    print(f\"Max length: {np.max(lengths)}\")\n",
    "    print(f\"Min length: {np.min(lengths)}\")\n",
    "    \n",
    "    # Check truncation at different max lengths\n",
    "    for test_max_len in [1024, 1536, 2048, 2560, 3072]:\n",
    "        truncated = np.sum(lengths > test_max_len)\n",
    "        pct_truncated = 100 * truncated / len(lengths)\n",
    "        pct_retained = 100 - pct_truncated\n",
    "        print(f\"At max_length={test_max_len}: {truncated}/{len(lengths)} truncated ({pct_truncated:.1f}%) | {pct_retained:.1f}% data retained\")\n",
    "    \n",
    "    return sequence_lengths\n",
    "\n",
    "# Run analysis on your train split\n",
    "print(\"=== SEQUENCE LENGTH ANALYSIS ===\")\n",
    "sequence_lengths = analyze_sequence_lengths(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790d99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf5d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_example(example):\n",
    "    # Clean the sequences\n",
    "    allowed_chars = \"ACDEFGHIKLMNPQRSTVWY[]@*/#\"\n",
    "    antigen = \"\".join([c for c in example['antigen'] if c in allowed_chars])\n",
    "    antibody = \"\".join([c for c in example['antibody'].replace('|','/') if c in allowed_chars])\n",
    "    \n",
    "    # Format: @ANTIGEN/ANTIBODY*\n",
    "    text = f\"@{antigen}/{antibody}*\"\n",
    "    \n",
    "    # Check length before truncation (optional - for monitoring)\n",
    "    full_tokens = tokenizer(text, add_special_tokens=False, truncation=False)\n",
    "    original_length = len(full_tokens[\"input_ids\"])\n",
    "    \n",
    "    # Tokenize with your settings\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_LEN,  # Use the optimal length from analysis\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    \n",
    "    # Create labels with masking (your current approach)\n",
    "    prompt_ids = tokenizer(text, add_special_tokens=False, truncation=True, max_length=MAX_LEN)[\"input_ids\"]\n",
    "    seq_len = len(enc[\"input_ids\"])\n",
    "    labels = [-100] * seq_len\n",
    "    \n",
    "    # Only label the actual sequence, not padding\n",
    "    for i in range(len(prompt_ids)):\n",
    "        if i < seq_len and enc[\"input_ids\"][i] != tokenizer.pad_token_id:\n",
    "            labels[i] = enc[\"input_ids\"][i]\n",
    "    \n",
    "    enc[\"labels\"] = labels\n",
    "    \n",
    "    # Optional: track truncation\n",
    "    if original_length > MAX_LEN:\n",
    "        enc[\"was_truncated\"] = True\n",
    "        enc[\"original_length\"] = original_length\n",
    "    \n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9669882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Length Analysis:\n",
      "Total sequences: 9523\n",
      "Mean length: 631.4\n",
      "Median length: 542.0\n",
      "Max length: 3560\n",
      "Min length: 224\n",
      "At max_length=1024: 1409/9523 truncated (14.8%) | 85.2% data retained\n",
      "At max_length=1536: 92/9523 truncated (1.0%) | 99.0% data retained\n",
      "At max_length=2048: 41/9523 truncated (0.4%) | 99.6% data retained\n",
      "At max_length=2560: 3/9523 truncated (0.0%) | 100.0% data retained\n",
      "At max_length=3072: 3/9523 truncated (0.0%) | 100.0% data retained\n",
      "Final tokenized dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 9523\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 1. Analyze sequence lengths first\n",
    "sequence_lengths = analyze_sequence_lengths(dataset['train'])\n",
    "\n",
    "# 2. Choose optimal MAX_LEN based on analysis (e.g., for 95% retention)\n",
    "MAX_LEN = 2048  # Adjust based on your analysis results\n",
    "\n",
    "# 3. Apply tokenization\n",
    "tokenized = dataset.map(build_example, remove_columns=dataset['train'].column_names)\n",
    "\n",
    "# 4. Check how many were actually truncated\n",
    "if 'was_truncated' in tokenized['train'][0]:\n",
    "    truncated_count = sum(1 for x in tokenized['train'] if x.get('was_truncated', False))\n",
    "    print(f\"Actually truncated during tokenization: {truncated_count}/{len(tokenized['train'])} ({100*truncated_count/len(tokenized['train']):.1f}%)\")\n",
    "\n",
    "print(\"Final tokenized dataset:\")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1005717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input id: 33\n",
      "Max label id: 33\n",
      "Vocab size: 36\n"
     ]
    }
   ],
   "source": [
    "test_antibody = \"EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLEWVASIYSYYGSTSYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCAREYHSYWSYSWWPRVGLDYWGQGTLVTVSS|AQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLLIYSASSLYSGVPSRFSGSRSGTDFTLTISSLQPEDFATYYCQQASLTALLTFGQGTKVEIK\"\n",
    "test_antigen = \"KIEEGKLVIWINGDKGYNGLAEVGKKFEKDTGIKVTVEHPDKLEEKFPQVAATGDGPDIIFWAHDRFGGYAQSGLLAEITPDKAFQDKL[Y][P][F]TW[D][A]VRYN[G]KLIAYPIAVEALSLIYNKDLLPNPPKTWEEIPALDKELKAKGKSALMFNLQEPYFTWPLIAADGGYAFK[Y]EN[G][K][Y]DIKDVGVDNAGAKAGLTFLVDLIKNKHMNADTDYSIAEAAFNKGETAMTINGPWAWSNIDTSKVNYGVTVLPTFKGQPSKPF[V]GVLSAGINAASPNKELAKEFLENYLLTDEGLEAVNKDKPLGAVALKSY[E][E]EL[A]KDPR[I][A]AT[M][E]N[A][Q][K][G][E][I]M[P]NIPQMSAFWYAVRTAVINAASGRQTVDEALKDAQTIIELYRQSLEIISRYLREQATGAADTAPMGATSRKALETLRRVGDGVQRNHETAFQGMLRKLDIKNEDDVKSLSRVMIHVFSDGVTNWGRIVTLISFGAFVAKHLKTINQESAIEPLAESITDVLVRTKRDWLVKQRGWDGFVEFF\"\n",
    "\n",
    "example = {\n",
    "    'antigen': test_antigen,\n",
    "    'antibody': test_antibody\n",
    "}\n",
    "\n",
    "ex = build_example(example)\n",
    "print(\"Max input id:\", max(ex[\"input_ids\"]))\n",
    "print(\"Max label id:\", max([i for i in ex[\"labels\"] if i != -100]))\n",
    "print(\"Vocab size:\", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7deb3357",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f398504",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    label_names=[\"labels\"],\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True, # old code torch.cuda.is_available(),\n",
    "    # bf16=torch.cuda.is_available(),\n",
    "    optim=\"adamw_torch\", # old code --> \"paged_adamw_8bit\" if USE_QLORA else \"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    "    seed=int(SEED),\n",
    "    dataloader_num_workers=8,        # Increase workers\n",
    "    dataloader_pin_memory=False,  # Try False first\n",
    "    # no_cuda=True ## To Test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dd8c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized['train'],\n",
    "#     data_collator=data_collator,\n",
    "#     # data_collator=sft_collator,\n",
    "# )\n",
    "\n",
    " \n",
    "# ## Set up SFTTrainer\n",
    "from trl import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    # data_collator=sft_collator,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    # callbacks=[test_callback],  # Log every 50 steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51259fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids cuda:0 torch.int64\n",
      "attention_mask cuda:0 torch.int64\n",
      "labels cuda:0 torch.int64\n"
     ]
    }
   ],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "    # print(batch.keys())      # e.g., dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
    "    # print(batch[\"input_ids\"].shape)\n",
    "    for key, tensor in batch.items():\n",
    "        print(key, tensor.device, tensor.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c574af57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch tensor devices ===\n",
      "input_ids: device=cuda:0, dtype=torch.int64, shape=torch.Size([1, 1024])\n",
      "attention_mask: device=cuda:0, dtype=torch.int64, shape=torch.Size([1, 1024])\n",
      "labels: device=cuda:0, dtype=torch.int64, shape=torch.Size([1, 1024])\n",
      "\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Get a single batch from your trainer\n",
    "batch = next(iter(trainer.get_train_dataloader()))\n",
    "\n",
    "print(\"=== Batch tensor devices ===\")\n",
    "for key, tensor in batch.items():\n",
    "    print(f\"{key}: device={tensor.device}, dtype={tensor.dtype}, shape={tensor.shape}\")\n",
    "\n",
    "# Check model device\n",
    "model_device = next(model.parameters()).device\n",
    "print(f\"\\nModel device: {model_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e872ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5772a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SPEED DIAGNOSTICS ===\n",
      "Dataset size: 9523\n",
      "Batch size per device: 1\n",
      "Gradient accumulation: 8\n",
      "Number of GPUs: 2\n",
      "Effective batch size: 16\n",
      "Steps per epoch: 595\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Before training, check current setup\n",
    "print(\"=== SPEED DIAGNOSTICS ===\")\n",
    "print(f\"Dataset size: {len(tokenized['train'])}\")\n",
    "print(f\"Batch size per device: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "effective_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * torch.cuda.device_count()\n",
    "print(f\"Effective batch size: {effective_batch_size}\")\n",
    "steps_per_epoch = len(tokenized['train']) // effective_batch_size\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "# Time the training\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "print(f\"\\nTraining completed in: {total_time:.2f} seconds\")\n",
    "print(f\"Steps per second: {steps_per_epoch / total_time:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8fcb7539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the trained model\n",
    "trainer.save_model(training_args.output_dir)\n",
    "print(f\"Model saved to {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ece34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference Test\n",
    "def generate_antibody(antigen: str, max_new_tokens: int = 700):\n",
    "    prompt = tokenizer.bos_token + antigen + tokenizer.sep_token\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    text = tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n",
    "    return text.split(tokenizer.sep_token, 1)[-1].rstrip(tokenizer.eos_token)\n",
    "\n",
    "\n",
    "test_antigen = \"KVFGRCELAAAM[K][R]HGL[D][N][Y]RG[Y][S]LG[N]WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA[K]KIVSDGNGMNAWVAWRNRCK[G][T][D]V[Q]AW[I][R]GCRL\"\n",
    "print(\"Generated antibody:\", generate_antibody(test_antigen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
