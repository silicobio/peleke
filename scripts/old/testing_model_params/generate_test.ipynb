{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87884c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 199.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antigen: DTICIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDSHNGKLCLLKGIAPLQLGNCSVAGWILGNPECELLISKESWSYIVETPNPENGTCYPGYFADYEELREQLSSVSSFERFEIFPKGSSWPNHTVTGVSASCSHNGKSSFYRNLLWLTGKNGLYPNLSMSYVNNKEKEVLVLWGVHHPPNIGDQRALYHTENAYVSVVSSHYSRRFTPEIAKRPKVRDQEGRINYYWTLLEPGDTIIFEANGNLIAPWYAFALSRGFGSGIITSNAPMDECDAKCQTPQGAINSSLPFQNVHPV\n",
      "Antibody: VQLQESGPGLVAPSQSLSITCTVSGFSLTFSSNYMWVRQAPGKGLEWLGRIYSGSTYRPSVTKISASTQSVYSLDTSKNQFSLKLTAEDTAVYYCARRGSGNYDYWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSS\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load model\n",
    "model_name = \"microsoft/phi-4\" #\"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\n",
    "model = PeftModel.from_pretrained(base_model, \"/home/nicholas/Documents/GitHub/peleke/models/peleke-phi-4/checkpoint-1434/\").cuda()\n",
    "model.eval()\n",
    "\n",
    "# Generate complete antibody sequences\n",
    "test_antigens = [\n",
    "    \"DTICIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDSHNGKLCLLKGIAPLQLGNCSVAGWILGNPECELLISKESWSYIVETPNPENGTCYPGYFADYEELREQLSSVSSFERFEIFPKGSSWPNHTVTGVSASCSHNGKSSFYRNLLWLTGKNGLYPNLSMSYVNNKEKEVLVLWGVHHPPNIGDQRALYHTENAYVSVVSSHYSRRFTPEIAKRPKVRDQEGRINYYWTLLEPGDTIIFEANGNLIAPWYAFALSRGFGSGIITSNAPMDECDAKCQTPQGAINSSLPFQNVHPV\",\n",
    " \n",
    "]\n",
    "\n",
    "for antigen in test_antigens:\n",
    "    prompt = f\"Antigen: {antigen}\\nAntibody: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=False,\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    antibody_sequence = full_text.replace(prompt, \"\").strip()\n",
    "    \n",
    "    print(f\"Antigen: {antigen}\")\n",
    "    print(f\"Antibody: {antibody_sequence}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfbd3a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DynamicCache' object has no attribute 'get_max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     outputs = \u001b[43mmodel_cpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Greedy decoding\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m full_text = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     17\u001b[39m antibody = full_text.replace(prompt, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/peft/peft_model.py:1875\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1873\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1874\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1875\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1877\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2623\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2615\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2616\u001b[39m         input_ids=input_ids,\n\u001b[32m   2617\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2618\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2619\u001b[39m         **model_kwargs,\n\u001b[32m   2620\u001b[39m     )\n\u001b[32m   2622\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2623\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2634\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2635\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2636\u001b[39m         input_ids=input_ids,\n\u001b[32m   2637\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2638\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2639\u001b[39m         **model_kwargs,\n\u001b[32m   2640\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3597\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3593\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3595\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   3596\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3597\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3599\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[32m   3600\u001b[39m     model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-mini-instruct/3145e03a9fd4cdd7cd953c34d9bbf7ad606122ca/modeling_phi3.py:1299\u001b[39m, in \u001b[36mPhi3ForCausalLM.prepare_inputs_for_generation\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, **kwargs)\u001b[39m\n\u001b[32m   1297\u001b[39m     cache_length = past_key_values.get_seq_length()\n\u001b[32m   1298\u001b[39m     past_length = past_key_values.seen_tokens\n\u001b[32m-> \u001b[39m\u001b[32m1299\u001b[39m     max_cache_length = \u001b[43mpast_key_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_max_length\u001b[49m()\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1301\u001b[39m     cache_length = past_length = past_key_values[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].shape[\u001b[32m2\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'DynamicCache' object has no attribute 'get_max_length'"
     ]
    }
   ],
   "source": [
    "# Test on CPU to avoid GPU issues\n",
    "model_cpu = model.cpu()\n",
    "\n",
    "test_antigen = \"DTICIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDSHNGKLCLLKGIAPLQLGNCSVAGWILGNPECELLISKESWSYIVETPNPENGTCYPGYFADYEELREQLSSVSSFERFEIFPKGSSWPNHTVTGVSASCSHNGKSSFYRNLLWLTGKNGLYPNLSMSYVNNKEKEVLVLWGVHHPPNIGDQRALYHTENAYVSVVSSHYSRRFTPEIAKRPKVRDQEGRINYYWTLLEPGDTIIFEANGNLIAPWYAFALSRGFGSGIITSNAPMDECDAKCQTPQGAINSSLPFQNVHPV\"\n",
    "prompt = f\"Antigen: {test_antigen}\\nAntibody: \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_cpu.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,  # Greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "antibody = full_text.replace(prompt, \"\").strip()\n",
    "\n",
    "print(f\"Antigen: {test_antigen}\")\n",
    "print(f\"Antibody: {antibody}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe778f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb</th>\n",
       "      <th>Hchain</th>\n",
       "      <th>Lchain</th>\n",
       "      <th>AntigenChains</th>\n",
       "      <th>HeavySeq</th>\n",
       "      <th>LightSeq</th>\n",
       "      <th>AntigenSeq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8xa4</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>A | B</td>\n",
       "      <td>QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERVTLSCRASQRVSSTYLAWYQQKPGQAPR...</td>\n",
       "      <td>SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9cph</td>\n",
       "      <td>H</td>\n",
       "      <td>L</td>\n",
       "      <td>A</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...</td>\n",
       "      <td>AQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLL...</td>\n",
       "      <td>KIEEGKLVIWINGDKGYNGLAEVGKKFEKDTGIKVTVEHPDKLEEK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9d7i</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "      <td>E</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9d7i</td>\n",
       "      <td>J</td>\n",
       "      <td>I</td>\n",
       "      <td>C</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9d7o</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "      <td>E</td>\n",
       "      <td>QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pdb Hchain Lchain AntigenChains  \\\n",
       "4   8xa4      C      D         A | B   \n",
       "9   9cph      H      L             A   \n",
       "10  9d7i      H      G             E   \n",
       "11  9d7i      J      I             C   \n",
       "12  9d7o      H      G             E   \n",
       "\n",
       "                                             HeavySeq  \\\n",
       "4   QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...   \n",
       "9   EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...   \n",
       "10  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "11  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "12  QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...   \n",
       "\n",
       "                                             LightSeq  \\\n",
       "4   EIVLTQSPGTLSLSPGERVTLSCRASQRVSSTYLAWYQQKPGQAPR...   \n",
       "9   AQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLL...   \n",
       "10  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "11  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "12  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "\n",
       "                                           AntigenSeq  \n",
       "4   SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWL...  \n",
       "9   KIEEGKLVIWINGDKGYNGLAEVGKKFEKDTGIKVTVEHPDKLEEK...  \n",
       "10  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...  \n",
       "11  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...  \n",
       "12  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "## Load dataset\n",
    "df = pd.read_csv(\"./data/sabdab/sabdab_with_sequences.tsv\", sep='\\t')\n",
    "\n",
    "## Remove rows with missing sequences\n",
    "df = df.dropna(subset=['HeavySeq', 'LightSeq', 'AntigenSeq'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4152b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.82it/s]\n",
      "Device set to use cuda:0\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mgenerate_antibody_sequence\u001b[39m\u001b[34m(antigen_sequence, model_path, base_model_name, max_new_tokens, top_p, temperature, top_k)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     outputs = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:316\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1464\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1471\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1470\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1471\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1371\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1370\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:414\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    412\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/peft/peft_model.py:1875\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1874\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1875\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2623\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2622\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2623\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2634\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3597\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3595\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   3596\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3597\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3599\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-mini-instruct/3145e03a9fd4cdd7cd953c34d9bbf7ad606122ca/modeling_phi3.py:1299\u001b[39m, in \u001b[36mPhi3ForCausalLM.prepare_inputs_for_generation\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, **kwargs)\u001b[39m\n\u001b[32m   1298\u001b[39m     past_length = past_key_values.seen_tokens\n\u001b[32m-> \u001b[39m\u001b[32m1299\u001b[39m     max_cache_length = \u001b[43mpast_key_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_max_length\u001b[49m()\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'DynamicCache' object has no attribute 'get_max_length'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:283\u001b[39m, in \u001b[36mBatchEncoding.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'cuda'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Generate antibodies\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m antigen \u001b[38;5;129;01min\u001b[39;00m test_antigens:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     antibody_sequence = \u001b[43mgenerate_antibody_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mantigen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mprint\u001b[39m(antibody_sequence)\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mgenerate_antibody_sequence\u001b[39m\u001b[34m(antigen_sequence, model_path, base_model_name, max_new_tokens, top_p, temperature, top_k)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Fallback if pipeline fails\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     inputs = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m()\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     50\u001b[39m         outputs = model.generate(\n\u001b[32m     51\u001b[39m             **inputs,\n\u001b[32m     52\u001b[39m             max_new_tokens=max_new_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m             pad_token_id=tokenizer.eos_token_id\n\u001b[32m     58\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:285\u001b[39m, in \u001b[36mBatchEncoding.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data[item]\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use 3090 Ti\n",
    "\n",
    "def generate_antibody_sequence(\n",
    "    antigen_sequence: str,\n",
    "    model_path: str = \"./phi35-antibody-corrected-multigpu\",\n",
    "    base_model_name: str = \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    max_new_tokens: int = 200,\n",
    "    top_p: float = 0.9,\n",
    "    temperature: float = 0.9,\n",
    "    top_k: int = 50\n",
    ") -> str:\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    \n",
    "    # Load base model + PEFT adapter\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name, \n",
    "        torch_dtype=torch.float32,  # Use float32 for compatibility\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.cuda()\n",
    "    \n",
    "    # Create pipeline\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "    \n",
    "    prompt = f\"Antigen: {antigen_sequence}\\nAntibody: \"\n",
    "    \n",
    "    try:\n",
    "        outputs = generator(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        return outputs[0][\"generated_text\"]\n",
    "    except:\n",
    "        # Fallback if pipeline fails\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test antigens\n",
    "test_antigens = [\"DTICIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDSHNGKLCLLKGIAPLQLGNCSVAGWILGNPECELLISKESWSYIVETPNPENGTCYPGYFADYEELREQLSSVSSFERFEIFPKGSSWPNHTVTGVSASCSHNGKSSFYRNLLWLTGKNGLYPNLSMSYVNNKEKEVLVLWGVHHPPNIGDQRALYHTENAYVSVVSSHYSRRFTPEIAKRPKVRDQEGRINYYWTLLEPGDTIIFEANGNLIAPWYAFALSRGFGSGIITSNAPMDECDAKCQTPQGAINSSLPFQNVHPV\"\n",
    "\n",
    "]\n",
    "    \n",
    "\n",
    "# Generate antibodies\n",
    "for antigen in test_antigens:\n",
    "    antibody_sequence = generate_antibody_sequence(antigen)\n",
    "    print(antibody_sequence)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f4114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.18it/s]\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antibody: LNQLQSCGGHALPGTSVRISCRASHNDPYWGQGLEYIGTIWHPNDGEGGDMTRNSSGWQSYDRGNNEQVEAQKRLSISISYGQGTDETDKEGCYLKWSTIGIYNQYSGSSMCEFKNVFKNDESSSTSTSLESLRTVASSLSSLQLTSGGGTGAASAVSVNNAKQTRKHRISYVPHNSWGVXSADAASWWADVFMHVTVCCJGDMRDKVERNYCSSHRTSRCSCLHDCLSCGPNGESPM\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "def generate_antibody_sequence(antigen_sequence: str) -> str:\n",
    "    # Load model\n",
    "    model_path = \"./phi35-antibody-corrected-multigpu\"\n",
    "    base_model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name, \n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate step by step\n",
    "    prompt = f\"Antigen: {antigen_sequence}\\nAntibody: \"\n",
    "    current_text = prompt\n",
    "    generated_part = \"\"\n",
    "    \n",
    "    for _ in range(150):  # Max 150 tokens\n",
    "        # Tokenize current text\n",
    "        inputs = tokenizer(current_text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # Get next token\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, 1)[0]\n",
    "            next_token = tokenizer.decode(next_token_id.item())\n",
    "            \n",
    "            # Stop conditions\n",
    "            if next_token in [tokenizer.eos_token, \"\\n\"] or next_token.strip() == \"\":\n",
    "                break\n",
    "                \n",
    "            # Add to sequence\n",
    "            current_text += next_token\n",
    "            generated_part += next_token\n",
    "            \n",
    "            # Stop if we have a complete antibody\n",
    "            if \"|\" in generated_part and len(generated_part) > 60:\n",
    "                break\n",
    "    \n",
    "    return f\"Antibody: {generated_part.strip()}\"\n",
    "\n",
    "# Test antigens\n",
    "test_antigens = [\"DTICIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDSHNGKLCLLKGIAPLQLGNCSVAGWILGNPECELLISKESWSYIVETPNPENGTCYPGYFADYEELREQLSSVSSFERFEIFPKGSSWPNHTVTGVSASCSHNGKSSFYRNLLWLTGKNGLYPNLSMSYVNNKEKEVLVLWGVHHPPNIGDQRALYHTENAYVSVVSSHYSRRFTPEIAKRPKVRDQEGRINYYWTLLEPGDTIIFEANGNLIAPWYAFALSRGFGSGIITSNAPMDECDAKCQTPQGAINSSLPFQNVHPV\"\n",
    "\n",
    "] \n",
    "\n",
    "# Generate antibodies\n",
    "for antigen in test_antigens:\n",
    "    result = generate_antibody_sequence(antigen)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4981d3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.11it/s]\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antibody: qlLSCSSILQESGGGLAAGMGTSLTTDQSPYFDVWHIHWVKQARNQGPGQSPGLEWIGNIYCGDGSTNQVKASLRSEYLQWNSVTTAPSVKTITRGRFTSADSFSVTAALHYVCGYWGQGTSVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYEAPHYWGQGTLVTVPSSSLT|RISMASSSVSGVF\n",
      "Antigen:DPTICIGYHANNSTDTVLATVTVSLHSLKDVLNGTYPVYPFRQQHAGSGTISSHGPGQCCPFSWVLLLQDQKLLLKGIAPLQLGVNFWLGIGWGNPECELLISKESWSYIVETPNPENGTCYPGYFAYEELREQLSSVSRFQHLTFESDEKQITTVYSVTVTKQKFRQPTLGGITPVVQTTIVTXIGCFTKAYVSVSSERFEIFPK\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_antibody_sequence(antigen_sequence: str) -> str:\n",
    "    import os\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Set environment to use both GPUs\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "    \n",
    "    # Load model\n",
    "    model_path = \"./phi35-antibody-corrected-multigpu\"\n",
    "    base_model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    \n",
    "    # Use the same device map as training\n",
    "    custom_device_map = {\n",
    "        'model.embed_tokens': 1,\n",
    "        'model.layers.0': 1, 'model.layers.1': 1, 'model.layers.2': 1, 'model.layers.3': 1,\n",
    "        'model.layers.4': 1, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1,\n",
    "        'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1,\n",
    "        'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1,\n",
    "        'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0,\n",
    "        'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0,\n",
    "        'model.layers.24': 0, 'model.layers.25': 0, 'model.layers.26': 0, 'model.layers.27': 0,\n",
    "        'model.layers.28': 0, 'model.layers.29': 0, 'model.layers.30': 0, 'model.layers.31': 0,\n",
    "        'model.norm': 1,\n",
    "        'lm_head': 1,\n",
    "    }\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name, \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=custom_device_map\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate step by step\n",
    "    prompt = f\"Antigen: {antigen_sequence}\\nAntibody: \"\n",
    "    current_text = prompt\n",
    "    generated_part = \"\"\n",
    "    \n",
    "    for step in range(300):\n",
    "        # Tokenize and put on GPU 1 (where embeddings are)\n",
    "        inputs = tokenizer(current_text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.cuda(1) for k, v in inputs.items()}  # Explicitly use GPU 1\n",
    "        \n",
    "        # Get next token\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            # Sample next token\n",
    "            temperature = 0.8\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, 1)[0]\n",
    "            next_token = tokenizer.decode(next_token_id.item())\n",
    "            \n",
    "            # Stopping conditions\n",
    "            if next_token in [tokenizer.eos_token, \"<|endoftext|>\"]:\n",
    "                break\n",
    "                \n",
    "            if next_token == \"\\n\":\n",
    "                if \"|\" in generated_part and len(generated_part.split(\"|\")) >= 2:\n",
    "                    if len(generated_part.split(\"|\")[1]) > 20:\n",
    "                        break\n",
    "            \n",
    "            # Add to sequence\n",
    "            current_text += next_token\n",
    "            generated_part += next_token\n",
    "            \n",
    "            # Force separator if needed\n",
    "            if step > 100 and \"|\" not in generated_part:\n",
    "                sep_token_id = tokenizer.encode(\"|\", add_special_tokens=False)[0]\n",
    "                separator_prob = torch.softmax(logits, dim=-1)[sep_token_id].item()\n",
    "                \n",
    "                if separator_prob > 0.01:\n",
    "                    current_text += \"|\"\n",
    "                    generated_part += \"|\"\n",
    "                    continue\n",
    "            \n",
    "            # Good stopping point\n",
    "            if \"|\" in generated_part:\n",
    "                parts = generated_part.split(\"|\")\n",
    "                if len(parts) >= 2 and len(parts[1]) > 50:\n",
    "                    break\n",
    "    \n",
    "    # Clean up\n",
    "    antibody_seq = generated_part.strip()\n",
    "    \n",
    "    if \"|\" not in antibody_seq:\n",
    "        if len(antibody_seq) > 100:\n",
    "            split_point = len(antibody_seq) // 2\n",
    "            antibody_seq = antibody_seq[:split_point] + \"|\" + antibody_seq[split_point:]\n",
    "        else:\n",
    "            antibody_seq += \"|LIGHTCHAIN\"\n",
    "    \n",
    "    return f\"Antibody: {antibody_seq}\"\n",
    "\n",
    "# Test antigens\n",
    "test_antigens = [\"DTICIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDSHNGKLCLLKGIAPLQLGNCSVAGWILGNPECELLISKESWSYIVETPNPENGTCYPGYFADYEELREQLSSVSSFERFEIFPKGSSWPNHTVTGVSASCSHNGKSSFYRNLLWLTGKNGLYPNLSMSYVNNKEKEVLVLWGVHHPPNIGDQRALYHTENAYVSVVSSHYSRRFTPEIAKRPKVRDQEGRINYYWTLLEPGDTIIFEANGNLIAPWYAFALSRGFGSGIITSNAPMDECDAKCQTPQGAINSSLPFQNVHPV\"\n",
    "\n",
    "] \n",
    "\n",
    "for antigen in test_antigens:\n",
    "    result = generate_antibody_sequence(antigen)\n",
    "    print(result)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0d5d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model predictions...\n",
      "Input: 'Antigen: MKFL\n",
      "Antibody: '\n",
      "Top 10 predictions:\n",
      "   1. '1' (score: 43.68)\n",
      "   2. '2' (score: 42.73)\n",
      "   3. '9' (score: 42.52)\n",
      "   4. '4' (score: 42.20)\n",
      "   5. '3' (score: 42.09)\n",
      "   6. '5' (score: 42.06)\n",
      "   7. '6' (score: 41.96)\n",
      "   8. '7' (score: 41.66)\n",
      "   9. '0' (score: 41.65)\n",
      "  10. '8' (score: 41.62)\n",
      "\n",
      "LoRA adapters enabled: ['default']\n",
      "\n",
      "Testing base model (no LoRA):\n",
      "  1. '1' (43.57)\n",
      "  2. '2' (42.61)\n",
      "  3. '9' (42.39)\n",
      "  4. '4' (42.07)\n",
      "  5. '3' (41.96)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Load model\n",
    "model_path = \"./phi35-antibody-lora-e10\"\n",
    "base_model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "model = model.cuda().eval()\n",
    "\n",
    "# Test with exact training format\n",
    "test_prompt = \"Antigen: MKFL\\nAntibody: \"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "print(\"Testing model predictions...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[0, -1, :]\n",
    "    \n",
    "    # Get top 10 most likely tokens\n",
    "    top_k = torch.topk(logits, 10)\n",
    "    \n",
    "    print(f\"Input: '{test_prompt}'\")\n",
    "    print(\"Top 10 predictions:\")\n",
    "    for i, (score, token_id) in enumerate(zip(top_k.values, top_k.indices)):\n",
    "        token = tokenizer.decode(token_id.item())\n",
    "        print(f\"  {i+1:2d}. '{token}' (score: {score.item():.2f})\")\n",
    "\n",
    "# Check if LoRA is actually enabled\n",
    "print(f\"\\nLoRA adapters enabled: {model.active_adapters}\")\n",
    "\n",
    "# Test without LoRA\n",
    "print(\"\\nTesting base model (no LoRA):\")\n",
    "model.disable_adapter_layers()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[0, -1, :]\n",
    "    top_k = torch.topk(logits, 5)\n",
    "    \n",
    "    for i, (score, token_id) in enumerate(zip(top_k.values, top_k.indices)):\n",
    "        token = tokenizer.decode(token_id.item())\n",
    "        print(f\"  {i+1}. '{token}' ({score.item():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0c4dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING TRAINING DATA ===\n",
      "Original training format:\n",
      "Antigen: SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWLIDYVEDTWGSDGNPITKTTSDYQDSDVSQEVRKYFC|SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWLIDYVEDTWGSDGNPITKTTSDYQDSDVSQEVRKYFC\n",
      "Antibody: QLQLQESGPGLVKPS\n",
      "\n",
      "Tokenized and decoded:\n",
      "Antigen: SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWLIDYVEDTWGSDGNPITKTTSDYQDSDVSQEVRKYFC|SCNGLYYQ\n",
      "\n",
      "=== AMINO ACID TOKENS ===\n",
      "A: [319]\n",
      "C: [315]\n",
      "D: [360]\n",
      "E: [382]\n",
      "F: [383]\n",
      "G: [402]\n",
      "H: [379]\n",
      "I: [306]\n",
      "K: [476]\n",
      "L: [365]\n",
      "M: [341]\n",
      "N: [405]\n",
      "P: [349]\n",
      "Q: [660]\n",
      "R: [390]\n",
      "S: [317]\n",
      "T: [323]\n",
      "V: [478]\n",
      "W: [399]\n",
      "Y: [612]\n",
      "'|': [891]\n"
     ]
    }
   ],
   "source": [
    "# Check what your training data actually looked like\n",
    "print(\"=== CHECKING TRAINING DATA ===\")\n",
    "sample_from_df = df.iloc[0]\n",
    "formatted_sample = f\"Antigen: {sample_from_df['AntigenSeq']}\\nAntibody: {sample_from_df['HeavySeq']}|{sample_from_df['LightSeq']}\\n\"\n",
    "print(\"Original training format:\")\n",
    "print(formatted_sample[:200])\n",
    "\n",
    "# Check how it was tokenized\n",
    "inputs = tokenizer(formatted_sample[:100], return_tensors=\"pt\")\n",
    "decoded = tokenizer.decode(inputs['input_ids'][0])\n",
    "print(f\"\\nTokenized and decoded:\")\n",
    "print(decoded[:200])\n",
    "\n",
    "# Check if amino acids are in vocabulary\n",
    "print(f\"\\n=== AMINO ACID TOKENS ===\")\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "for aa in amino_acids:\n",
    "    token_id = tokenizer.encode(aa, add_special_tokens=False)\n",
    "    print(f\"{aa}: {token_id}\")\n",
    "\n",
    "# Check separator\n",
    "sep_token = tokenizer.encode(\"|\", add_special_tokens=False)\n",
    "print(f\"'|': {sep_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c7f010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIXING TRAINING DATA ===\n",
      "Original data sample:\n",
      "AntigenSeq: SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWLIDYV...\n",
      "HeavySeq: QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKGLEWI...\n",
      "LightSeq: EIVLTQSPGTLSLSPGERVTLSCRASQRVSSTYLAWYQQKPGQAPRLLIY...\n",
      "\n",
      "Corrected format:\n",
      "Antigen: SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWLIDYVEDTWGSDGNPITKTTSDYQDSDVSQEVRKYFC|SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWLIDYVEDTWGSDGNPITKTTSDYQDSDVSQEVRKYFC\n",
      "Antibody: QLQLQESGPGLVKPS\n",
      "\n",
      "=== RETRAINING WITH CORRECT FORMAT ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10073/10073 [00:00<00:00, 36494.14 examples/s]\n",
      "Map: 100%|██████████| 10073/10073 [00:03<00:00, 3085.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected dataset size: 8058 train, 2015 eval\n",
      "Ready to retrain with correct format!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Fix the training data format\n",
    "def format_prompt_correct(example):\n",
    "    return {\n",
    "        \"text\": f\"Antigen: {example['AntigenSeq']}\\nAntibody: {example['HeavySeq']}|{example['LightSeq']}\\n\"\n",
    "    }\n",
    "\n",
    "print(\"=== FIXING TRAINING DATA ===\")\n",
    "\n",
    "# Check original data\n",
    "print(\"Original data sample:\")\n",
    "sample = df.iloc[0]\n",
    "print(f\"AntigenSeq: {sample['AntigenSeq'][:50]}...\")\n",
    "print(f\"HeavySeq: {sample['HeavySeq'][:50]}...\")  \n",
    "print(f\"LightSeq: {sample['LightSeq'][:50]}...\")\n",
    "\n",
    "# Create corrected format\n",
    "corrected_sample = format_prompt_correct(sample)\n",
    "print(f\"\\nCorrected format:\")\n",
    "print(corrected_sample['text'][:200])\n",
    "\n",
    "# Recreate dataset with correct format\n",
    "print(\"\\n=== RETRAINING WITH CORRECT FORMAT ===\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_prompt_correct)\n",
    "\n",
    "def tokenize(example):\n",
    "    encoded = tokenizer(\n",
    "        example[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=128\n",
    "    )\n",
    "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    return encoded\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\n",
    "    'pdb', 'Hchain', 'Lchain', 'AntigenSeq', 'AntigenChains',\n",
    "    'HeavySeq', 'LightSeq', '__index_level_0__', 'text'\n",
    "])\n",
    "\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=1337)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Corrected dataset size: {len(train_dataset)} train, {len(eval_dataset)} eval\")\n",
    "print(\"Ready to retrain with correct format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00260a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RETRAINING WITH CORRECT FORMAT ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CORRECTED PEFT MODEL INFO ===\n",
      "trainable params: 206,438,400 || all params: 3,830,516,736 || trainable%: 5.3893\n",
      "Starting corrected training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='359' max='5040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 359/5040 03:36 < 47:17, 1.65 it/s, Epoch 0.36/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 111\u001b[39m\n\u001b[32m    108\u001b[39m gc.collect()\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting corrected training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Save the corrected model\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaving corrected model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/trainer.py:2207\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2205\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/trainer.py:2549\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2542\u001b[39m context = (\n\u001b[32m   2543\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2544\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2546\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2547\u001b[39m )\n\u001b[32m   2548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2552\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2554\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2555\u001b[39m ):\n\u001b[32m   2556\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2557\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/trainer.py:3798\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3796\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3798\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3800\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:2553\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2553\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/torch/_tensor.py:653\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    645\u001b[39m         Tensor.backward,\n\u001b[32m    646\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    651\u001b[39m         inputs=inputs,\n\u001b[32m    652\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Use both GPUs if available\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import gc\n",
    "\n",
    "# Clear memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"=== RETRAINING WITH CORRECT FORMAT ===\")\n",
    "\n",
    "# Load fresh model (don't use the incorrectly trained one)\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add amino acid tokens\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "extra_tokens = amino_acids + [\"|\"]\n",
    "new_tokens = [t for t in extra_tokens if t not in tokenizer.get_vocab()]\n",
    "if new_tokens:\n",
    "    tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Load fresh base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",  # Automatically distribute across GPUs\n",
    ").cuda()\n",
    "\n",
    "# Resize embeddings\n",
    "if new_tokens:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# PEFT configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # Increased from 4\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"o_proj\", \"qkv_proj\"],  # Target more modules\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.train()\n",
    "\n",
    "# Enable gradients for embeddings\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'embed_tokens' in name or 'lm_head' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(\"=== CORRECTED PEFT MODEL INFO ===\")\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Fixed training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi35-antibody-corrected\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,  # Train for 5 epochs\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
    "    learning_rate=5e-5,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "        # Multi-GPU specific settings\n",
    "    dataloader_num_workers=2,  # More workers for multi-GPU\n",
    "    ddp_find_unused_parameters=False,  # Optimization for LoRA\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Clean up memory\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Starting corrected training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the corrected model\n",
    "print(\"Saving corrected model...\")\n",
    "peft_model.save_pretrained(\"./phi35-antibody-corrected\")\n",
    "tokenizer.save_pretrained(\"./phi35-antibody-corrected\")\n",
    "print(\"✓ Corrected model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf91d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING GPU COMPATIBILITY ===\n",
      "Available GPUs: 2\n",
      "GPU 0: NVIDIA GeForce RTX 5090\n",
      "GPU 1: NVIDIA GeForce RTX 3090 Ti\n",
      "✓ GPU 0 working\n",
      "✓ GPU 1 working\n",
      "=== LOADING MODEL WITH CUSTOM DEVICE MAP ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Custom device map successful\n",
      "Model device placement:\n",
      "model.embed_tokens.weight: cuda:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm()\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Both GPUs visible\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import gc\n",
    "\n",
    "print(\"=== CHECKING GPU COMPATIBILITY ===\")\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "# Test both GPUs\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    try:\n",
    "        test_tensor = torch.randn(10, 10).cuda(i)\n",
    "        print(f\"✓ GPU {i} working\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ GPU {i} failed: {e}\")\n",
    "\n",
    "# Since RTX 5090 has issues, let's use a custom device map that prioritizes the 3090 Ti\n",
    "custom_device_map = {\n",
    "    'model.embed_tokens': 1,  # 3090 Ti\n",
    "    'model.layers.0': 1,      # Start layers on 3090 Ti\n",
    "    'model.layers.1': 1,\n",
    "    'model.layers.2': 1,\n",
    "    'model.layers.3': 1,\n",
    "    'model.layers.4': 1,\n",
    "    'model.layers.5': 1,\n",
    "    'model.layers.6': 1,\n",
    "    'model.layers.7': 1,\n",
    "    'model.layers.8': 1,\n",
    "    'model.layers.9': 1,\n",
    "    'model.layers.10': 1,\n",
    "    'model.layers.11': 1,\n",
    "    'model.layers.12': 1,\n",
    "    'model.layers.13': 1,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 0,     # Try some layers on 5090\n",
    "    'model.layers.17': 0,\n",
    "    'model.layers.18': 0,\n",
    "    'model.layers.19': 0,\n",
    "    'model.layers.20': 0,\n",
    "    'model.layers.21': 0,\n",
    "    'model.layers.22': 0,\n",
    "    'model.layers.23': 0,\n",
    "    'model.layers.24': 0,\n",
    "    'model.layers.25': 0,\n",
    "    'model.layers.26': 0,\n",
    "    'model.layers.27': 0,\n",
    "    'model.layers.28': 0,\n",
    "    'model.layers.29': 0,\n",
    "    'model.layers.30': 0,\n",
    "    'model.layers.31': 0,\n",
    "    'model.norm': 1,          # 3090 Ti\n",
    "    'lm_head': 1,            # 3090 Ti\n",
    "}\n",
    "\n",
    "# Clear memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"=== LOADING MODEL WITH CUSTOM DEVICE MAP ===\")\n",
    "\n",
    "# Load fresh model\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add amino acid tokens\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "extra_tokens = amino_acids + [\"|\"]\n",
    "new_tokens = [t for t in extra_tokens if t not in tokenizer.get_vocab()]\n",
    "if new_tokens:\n",
    "    tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "try:\n",
    "    # Try custom device map\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=custom_device_map,\n",
    "    )\n",
    "    print(\"✓ Custom device map successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Custom device map failed: {e}\")\n",
    "    print(\"Falling back to 3090 Ti only...\")\n",
    "    # Fallback to just 3090 Ti\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    ).cuda()\n",
    "\n",
    "# Resize embeddings\n",
    "if new_tokens:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Check where model parts are\n",
    "print(\"Model device placement:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if any(x in name for x in ['embed_tokens', 'layers.0', 'layers.16', 'layers.31', 'lm_head']):\n",
    "        print(f\"{name}: {param.device}\")\n",
    "        break\n",
    "\n",
    "# Continue with PEFT setup...\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"o_proj\", \"qkv_proj\"],\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.train()\n",
    "\n",
    "# Continue with training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee601078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RECREATING CORRECTED DATASET ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10073/10073 [00:00<00:00, 30377.79 examples/s]\n",
      "Map: 100%|██████████| 10073/10073 [00:02<00:00, 4231.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset recreated: 8058 train, 2015 eval\n",
      "=== CORRECTED PEFT MODEL INFO ===\n",
      "trainable params: 206,438,400 || all params: 3,830,516,736 || trainable%: 5.3893\n",
      "\n",
      "LoRA adapter device placement:\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Recreate the corrected dataset\n",
    "print(\"=== RECREATING CORRECTED DATASET ===\")\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Fix the training data format\n",
    "def format_prompt_correct(example):\n",
    "    return {\n",
    "        \"text\": f\"Antigen: {example['AntigenSeq']}\\nAntibody: {example['HeavySeq']}|{example['LightSeq']}\\n\"\n",
    "    }\n",
    "\n",
    "# Create corrected dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_prompt_correct)\n",
    "\n",
    "def tokenize(example):\n",
    "    encoded = tokenizer(\n",
    "        example[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=128\n",
    "    )\n",
    "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    return encoded\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\n",
    "    'pdb', 'Hchain', 'Lchain', 'AntigenSeq', 'AntigenChains',\n",
    "    'HeavySeq', 'LightSeq', '__index_level_0__', 'text'\n",
    "])\n",
    "\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=1337)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Dataset recreated: {len(train_dataset)} train, {len(eval_dataset)} eval\")\n",
    "\n",
    "# Enable gradients for embeddings\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'embed_tokens' in name or 'lm_head' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(\"=== CORRECTED PEFT MODEL INFO ===\")\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Check which devices the LoRA adapters are on\n",
    "print(\"\\nLoRA adapter device placement:\")\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'lora_A' in name or 'lora_B' in name:\n",
    "        print(f\"{name}: {param.device}\")\n",
    "        break\n",
    "\n",
    "# Continue with the rest of your training code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8343e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OPTIMIZED TRAINING SETTINGS ===\n",
      "Effective batch size: 32 = 32\n",
      "Total steps per epoch: 251\n",
      "Starting fast training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal steps per epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;250m \u001b[39m//\u001b[38;5;250m \u001b[39m(\u001b[32m16\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39mtorch.cuda.device_count())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting fast training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[43mtrainer\u001b[49m.train()\n\u001b[32m     63\u001b[39m end_time = time.time()\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time)/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Optimized training arguments for speed\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi35-antibody-corrected-multigpu\",\n",
    "    \n",
    "    # INCREASE BATCH SIZES - you have tons of VRAM left\n",
    "    per_device_train_batch_size=16,  # Increased from 6\n",
    "    per_device_eval_batch_size=16,   # Increased from 6\n",
    "    gradient_accumulation_steps=1,   # Reduced since batch size is larger\n",
    "    \n",
    "    # REDUCE EPOCHS - your dataset is small enough\n",
    "    num_train_epochs=3,  # Reduced from 5\n",
    "    \n",
    "    # SPEED OPTIMIZATIONS\n",
    "    max_steps=None,  # Let it run full epochs\n",
    "    warmup_steps=25,  # Reduced from 50\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,  # More frequent logging to monitor\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\",  # Disable evaluation during training for speed\n",
    "    learning_rate=1e-4,  # Slightly higher learning rate\n",
    "    \n",
    "    # MEMORY/SPEED OPTIMIZATIONS\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=False,  # Disable to trade memory for speed\n",
    "    dataloader_pin_memory=True,    # Enable for faster data loading\n",
    "    dataloader_num_workers=4,      # More workers for faster data loading\n",
    "    \n",
    "    # OTHER OPTIMIZATIONS\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,  # Disable for speed\n",
    "    ddp_find_unused_parameters=False,\n",
    "    \n",
    "    # ADDITIONAL SPEED SETTINGS\n",
    "    group_by_length=True,  # Group similar length sequences\n",
    "    length_column_name=\"length\",  # If you have length info\n",
    "    optim=\"adamw_torch_fused\",  # Faster optimizer\n",
    ")\n",
    "\n",
    "# Also increase LoRA rank since you have memory headroom\n",
    "peft_config = LoraConfig(\n",
    "    r=32,  # Increased from 16 - more capacity\n",
    "    lora_alpha=64,  # Increased proportionally\n",
    "    lora_dropout=0.05,  # Reduced dropout\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"o_proj\", \"qkv_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # Target more modules\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Monitor training speed\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"=== OPTIMIZED TRAINING SETTINGS ===\")\n",
    "print(f\"Effective batch size: {16 * torch.cuda.device_count() * 1} = {16 * torch.cuda.device_count()}\")\n",
    "print(f\"Total steps per epoch: {len(train_dataset) // (16 * torch.cuda.device_count())}\")\n",
    "print(\"Starting fast training...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {(end_time - start_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8808f7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CORRECTED PEFT MODEL INFO ===\n",
      "trainable params: 206,438,400 || all params: 3,830,516,736 || trainable%: 5.3893\n",
      "\n",
      "LoRA adapter device placement:\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "\n",
      "GPU usage before training:\n",
      "GPU 0: 3.7GB allocated, 10.6GB reserved, 33.7GB total\n",
      "GPU 1: 5.3GB allocated, 11.5GB reserved, 25.3GB total\n",
      "\n",
      "=== STARTING CORRECTED MULTI-GPU TRAINING ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='930' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [930/930 12:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.878100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.840800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.927800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.816600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.850600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.661900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.492600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.473700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.590100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.497500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.558300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.508700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.103200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.932600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.369600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.078800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.087400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.063700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.253700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.833800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.967200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.952700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.834200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.972700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.798300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.888500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.793900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.087400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.944200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.879300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.910300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.843800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.723700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU usage after training:\n",
      "GPU 0: 3.7GB allocated, 15.1GB reserved, 33.7GB total\n",
      "GPU 1: 5.7GB allocated, 16.6GB reserved, 25.3GB total\n",
      "Saving corrected model...\n",
      "✓ Corrected model saved!\n"
     ]
    }
   ],
   "source": [
    "# Enable gradients for embeddings\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'embed_tokens' in name or 'lm_head' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(\"=== CORRECTED PEFT MODEL INFO ===\")\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Check which devices the LoRA adapters are on\n",
    "print(\"\\nLoRA adapter device placement:\")\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'lora_A' in name or 'lora_B' in name:\n",
    "        print(f\"{name}: {param.device}\")\n",
    "        break\n",
    "\n",
    "# Optimized training arguments for speed\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi35-antibody-corrected-multigpu\",\n",
    "    \n",
    "    # INCREASE BATCH SIZES - you have tons of VRAM left\n",
    "    per_device_train_batch_size=26,  # Increased from 6\n",
    "    per_device_eval_batch_size=26,   # Increased from 6\n",
    "    gradient_accumulation_steps=1,   # Reduced since batch size is larger\n",
    "    \n",
    "    # REDUCE EPOCHS - your dataset is small enough\n",
    "    num_train_epochs=3,  # Reduced from 5\n",
    "    \n",
    "    # SPEED OPTIMIZATIONS\n",
    " \n",
    "    warmup_steps=25,  # Reduced from 50\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,  # More frequent logging to monitor\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\",  # Disable evaluation during training for speed\n",
    "    learning_rate=1e-4,  # Slightly higher learning rate\n",
    "    \n",
    "    # MEMORY/SPEED OPTIMIZATIONS\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=False,  # Disable to trade memory for speed\n",
    "    dataloader_pin_memory=True,    # Enable for faster data loading\n",
    "    dataloader_num_workers=4,      # More workers for faster data loading\n",
    "    \n",
    "    # OTHER OPTIMIZATIONS\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,  # Disable for speed\n",
    "    ddp_find_unused_parameters=False,\n",
    "    \n",
    "    # ADDITIONAL SPEED SETTINGS\n",
    "    group_by_length=True,  # Group similar length sequences\n",
    "    length_column_name=\"length\",  # If you have length info\n",
    "    optim=\"adamw_torch_fused\",  # Faster optimizer\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Monitor GPU usage during training\n",
    "def print_gpu_usage():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        print(f\"GPU {i}: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved, {total:.1f}GB total\")\n",
    "\n",
    "print(\"\\nGPU usage before training:\")\n",
    "print_gpu_usage()\n",
    "\n",
    "# Clean up memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n=== STARTING CORRECTED MULTI-GPU TRAINING ===\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nGPU usage after training:\")\n",
    "print_gpu_usage()\n",
    "\n",
    "# Save the corrected model\n",
    "print(\"Saving corrected model...\")\n",
    "peft_model.save_pretrained(\"./phi35-antibody-corrected-multigpu\")\n",
    "tokenizer.save_pretrained(\"./phi35-antibody-corrected-multigpu\")\n",
    "print(\"✓ Corrected model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
