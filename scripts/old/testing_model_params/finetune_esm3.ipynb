{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3deb958f",
   "metadata": {},
   "source": [
    "# Fine-Tune an LLM for Antibody Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920184da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f381adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA GeForce GTX TITAN X\n",
      "Memory: 12.9 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "# import wandb\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Test your GPU setup\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3861480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pdb_id', 'h_chain_id', 'l_chain_id', 'antigen_ids', 'h_chain_seq',\n",
       "       'l_chain_seq', 'antigen_seqs', 'antibody_seqs', 'h_chain_fv_seq',\n",
       "       'l_chain_fv_seq', 'antibody_fv_seqs', 'highlighted_epitope_seqs',\n",
       "       'epitope_residues'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load dataset\n",
    "df = pd.read_csv(\"../data/sabdab/sabdab_training_dataset.csv\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8023680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>h_chain_id</th>\n",
       "      <th>l_chain_id</th>\n",
       "      <th>antigen_ids</th>\n",
       "      <th>h_chain_seq</th>\n",
       "      <th>l_chain_seq</th>\n",
       "      <th>antigen_seqs</th>\n",
       "      <th>antibody_seqs</th>\n",
       "      <th>h_chain_fv_seq</th>\n",
       "      <th>l_chain_fv_seq</th>\n",
       "      <th>antibody_fv_seqs</th>\n",
       "      <th>highlighted_epitope_seqs</th>\n",
       "      <th>epitope_residues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8xa4</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>A|B</td>\n",
       "      <td>QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERVTLSCRASQRVSSTYLAWYQQKPGQAPR...</td>\n",
       "      <td>SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWL...</td>\n",
       "      <td>QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...</td>\n",
       "      <td>QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERVTLSCRASQRVSSTYLAWYQQKPGQAPR...</td>\n",
       "      <td>QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...</td>\n",
       "      <td>SCNGLYYQGSCYI[L]HSD[Y]KSFEDAKANCAAESSTLPNKSDVL...</td>\n",
       "      <td>A:ARG 176|A:ASP 146|A:ASP 150|A:ASP 170|A:GLN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9cph</td>\n",
       "      <td>H</td>\n",
       "      <td>L</td>\n",
       "      <td>A</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...</td>\n",
       "      <td>AQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLL...</td>\n",
       "      <td>KIEEGKLVIWINGDKGYNGLAEVGKKFEKDTGIKVTVEHPDKLEEK...</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...</td>\n",
       "      <td>AQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLL...</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...</td>\n",
       "      <td>KIEEGKLVIWINGDKGYNGLAEVGKKFEKDTGIKVTVEHPDKLEEK...</td>\n",
       "      <td>A:ALA 1116|A:ALA 1122|A:ALA 1128|A:ALA 900|A:A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9d7i</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "      <td>E</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "      <td>E:ARG 429|E:ARG 469|E:ASN 177|E:ASN 197|E:ASN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9d7i</td>\n",
       "      <td>J</td>\n",
       "      <td>I</td>\n",
       "      <td>C</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "      <td>C:ARG 469|C:ASN 197|C:ASN 280|C:ASN 425|C:ASP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9d7o</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "      <td>E</td>\n",
       "      <td>QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "      <td>QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...</td>\n",
       "      <td>QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "      <td>E:ARG 429|E:ARG 469|E:ASN 197|E:ASN 280|E:ASN ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdb_id h_chain_id l_chain_id antigen_ids  \\\n",
       "0   8xa4          C          D         A|B   \n",
       "1   9cph          H          L           A   \n",
       "2   9d7i          H          G           E   \n",
       "3   9d7i          J          I           C   \n",
       "4   9d7o          H          G           E   \n",
       "\n",
       "                                         h_chain_seq  \\\n",
       "0  QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...   \n",
       "1  EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...   \n",
       "2  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "3  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "4  QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...   \n",
       "\n",
       "                                         l_chain_seq  \\\n",
       "0  EIVLTQSPGTLSLSPGERVTLSCRASQRVSSTYLAWYQQKPGQAPR...   \n",
       "1  AQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLL...   \n",
       "2  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "3  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "4  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "\n",
       "                                        antigen_seqs  \\\n",
       "0  SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWL...   \n",
       "1  KIEEGKLVIWINGDKGYNGLAEVGKKFEKDTGIKVTVEHPDKLEEK...   \n",
       "2  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...   \n",
       "3  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...   \n",
       "4  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...   \n",
       "\n",
       "                                       antibody_seqs  \\\n",
       "0  QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...   \n",
       "1  EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...   \n",
       "2  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "3  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "4  QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...   \n",
       "\n",
       "                                      h_chain_fv_seq  \\\n",
       "0  QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...   \n",
       "1  EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...   \n",
       "2  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "3  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "4  QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...   \n",
       "\n",
       "                                      l_chain_fv_seq  \\\n",
       "0  EIVLTQSPGTLSLSPGERVTLSCRASQRVSSTYLAWYQQKPGQAPR...   \n",
       "1  AQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLL...   \n",
       "2  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "3  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "4  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "\n",
       "                                    antibody_fv_seqs  \\\n",
       "0  QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...   \n",
       "1  EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...   \n",
       "2  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "3  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "4  QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...   \n",
       "\n",
       "                            highlighted_epitope_seqs  \\\n",
       "0  SCNGLYYQGSCYI[L]HSD[Y]KSFEDAKANCAAESSTLPNKSDVL...   \n",
       "1  KIEEGKLVIWINGDKGYNGLAEVGKKFEKDTGIKVTVEHPDKLEEK...   \n",
       "2  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...   \n",
       "3  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...   \n",
       "4  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...   \n",
       "\n",
       "                                    epitope_residues  \n",
       "0  A:ARG 176|A:ASP 146|A:ASP 150|A:ASP 170|A:GLN ...  \n",
       "1  A:ALA 1116|A:ALA 1122|A:ALA 1128|A:ALA 900|A:A...  \n",
       "2  E:ARG 429|E:ARG 469|E:ASN 177|E:ASN 197|E:ASN ...  \n",
       "3  C:ARG 469|C:ASN 197|C:ASN 280|C:ASN 425|C:ASP ...  \n",
       "4  E:ARG 429|E:ARG 469|E:ASN 197|E:ASN 280|E:ASN ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove rows with missing sequences\n",
    "df = df.dropna(subset=['h_chain_seq', 'l_chain_seq', 'antigen_seqs', 'highlighted_epitope_seqs'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17677f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colby\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Colby\\.cache\\huggingface\\hub\\models--EvolutionaryScale--esm3-sm-open-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Load base tokenizer and model FIRST\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvolutionaryScale/esm3-sm-open-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      5\u001b[0m     model_name,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# device_map= {'model.embed_tokens': 0, \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# max_memory={0: \"30GB\", 1:\"23.5GB\"}  # Limit GPU memory usage to 20GB\u001b[39;00m\n\u001b[0;32m     54\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1071\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1067\u001b[0m             config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m   1068\u001b[0m                 pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1069\u001b[0m             )\n\u001b[0;32m   1070\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m-> 1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[0;32m   1072\u001b[0m         tokenizer_auto_map \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mauto_map[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1074\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m tokenizer_auto_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2014\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2010\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2011\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[1;32m-> 2014\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2015\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2016\u001b[0m     init_configuration,\n\u001b[0;32m   2017\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2018\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2019\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2020\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2021\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2022\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2023\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2024\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2025\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2260\u001b[0m, in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2259\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[1;32m-> 2260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[0;32m   2261\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2264\u001b[0m     )\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\esm\\tokenization_esm.py:53\u001b[0m, in \u001b[0;36mEsmTokenizer.__init__\u001b[1;34m(self, vocab_file, unk_token, cls_token, pad_token, mask_token, eos_token, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     vocab_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     52\u001b[0m ):\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mload_vocab_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id_to_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens))\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_to_id \u001b[38;5;241m=\u001b[39m {tok: ind \u001b[38;5;28;01mfor\u001b[39;00m ind, tok \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens)}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\esm\\tokenization_esm.py:30\u001b[0m, in \u001b[0;36mload_vocab_file\u001b[1;34m(vocab_file)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_vocab_file\u001b[39m(vocab_file):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     31\u001b[0m         lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [l\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lines]\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "## Load base tokenizer and model FIRST\n",
    "# model_name = \"EvolutionaryScale/esm3-sm-open-v1\"\n",
    "model_name = \"facebook/esm2_t30_150M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16, # Load model in bfloat16 for better performance\n",
    "    low_cpu_mem_usage=True, # Reduce CPU memory usage during loading\n",
    "    # max_memory={0: \"30GB\", 1:\"23.5GB\"}  # Limit GPU memory usage to 20GB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aeef2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0, 'model.layers.24': 0, 'model.layers.25': 0, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "# Check current device mapping\n",
    "print(\"Current device map:\", model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6a44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add epitope tokens\n",
    "epitope_tokens = [\"<epi>\", \"</epi>\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": epitope_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989b8e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(100354, 5120, padding_idx=100349)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (qkv_proj): Linear(in_features=5120, out_features=7680, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=5120, out_features=35840, bias=False)\n",
       "          (down_proj): Linear(in_features=17920, out_features=5120, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((5120,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=100354, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add amino acid tokens\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "extra_tokens = amino_acids + [\"|\"]\n",
    "new_tokens = [t for t in extra_tokens if t not in tokenizer.get_vocab()]\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Resize model embeddings ONCE after adding all tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "018d2119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9529/9529 [00:00<00:00, 20998.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert epitope format function\n",
    "import re\n",
    "def convert_epitope_format(sequence):\n",
    "    return re.sub(r'\\[([A-Z])\\]', r'<epi>\\1</epi>', sequence)\n",
    "\n",
    "## NOW create dataset with all tokens available\n",
    "def format_prompt(example):\n",
    "    epitope_seq = convert_epitope_format(example['highlighted_epitope_sequences'])\n",
    "    return {\n",
    "        \"text\": f\"Antigen: {epitope_seq}<|im_end|>\\nAntibody: {example['antibody_sequences']}<|im_end|>\\n\"\n",
    "    }\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3807a8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100357, 5120, padding_idx=100349)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add task-specific tokens\n",
    "task_tokens = [\"Antigen\", \"Antibody\", \"Epitope\"]\n",
    "tokenizer.add_tokens(task_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd7f786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9529/9529 [00:02<00:00, 3272.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the dataset\n",
    "def tokenize(example):\n",
    "    encoded = tokenizer(example[\"text\"], truncation=True, max_length=1024)\n",
    "    # Make sure labels are a proper list, not nested\n",
    "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    return encoded\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de4ecba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized text:\n",
      "['Antigen', ':', 'ĠSCN', 'GL', 'YY', 'Q', 'G', 'SC', 'Y', 'I', '<epi>', 'L', '</epi>', 'H', 'SD', '<epi>', 'Y', '</epi>', 'K', 'SF', 'ED', 'AK', 'AN', 'CAA', 'ES', 'ST', 'LP', 'NK', 'SD', 'VL', 'TT', 'W', 'LI', '<epi>', 'D', '</epi>', '<epi>', 'Y', '</epi>', 'V', '<epi>', 'E', '</epi>', '<epi>', 'D', '</epi>', '<epi>', 'T', '</epi>', 'WG', 'SD', 'GN', 'P', 'IT', 'K', 'TT', 'SD', '<epi>', 'Y', '</epi>', 'Q', 'DS', '<epi>', 'D', '</epi>', 'VS', '<epi>', 'Q', '</epi>', '<epi>', 'E']\n"
     ]
    }
   ],
   "source": [
    "# Verify tokenization is working with epitope tokens\n",
    "print(\"Sample tokenized text:\")\n",
    "sample_tokens = tokenizer.tokenize(dataset[0]['text'][:200])\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb594211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\n",
    "    'pdb_id', 'h_chain_id', 'l_chain_id', 'antigen_ids', 'antigen_seqs',\n",
    "    'h_chain_seq', 'l_chain_seq', 'antibody_sequences',\n",
    "    'highlighted_epitope_sequences', 'epitope_residues', 'text'\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6fa4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the gradient fix to your model\n",
    "if hasattr(model, 'enable_input_require_grads'):\n",
    "    model.enable_input_require_grads()\n",
    "else:\n",
    "    def make_inputs_require_grad(module, input, output):\n",
    "        output.requires_grad_(True)\n",
    "    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0eecbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data collator\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=False,\n",
    "#     return_tensors=\"pt\",\n",
    "#     pad_to_multiple_of=8, # Pad to multiple of 8 for better performance on GPUs\n",
    "# )\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12048eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,372,800 || all params: 14,666,931,200 || trainable%: 0.0503\n"
     ]
    }
   ],
   "source": [
    " # Configure LoRA\n",
    "## PEFT configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    # target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    target_modules=[\"o_proj\", \"qkv_proj\"],\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbabff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update training arguments to enable wandb logging\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../models/peleke-{model_name.split('/')[-1]}-07222025\",\n",
    "    per_device_train_batch_size=9,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=25,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    logging_dir=\"../logs\",\n",
    "    logging_steps=25,\n",
    "    gradient_checkpointing=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",  # Enable wandb reporting\n",
    "    run_name=f\"lora-epitope-{model_name.split('/')[-1]}\",  # Run name for wandb\n",
    "    # optim=\"adamw_torch\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    dataloader_num_workers=8,  # Add parallel data loading\n",
    "    dataloader_pin_memory=True,  # Pin memory for faster data loading\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc18da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnicholas1-santolla\u001b[0m to \u001b[32mhttp://192.168.0.24:8080\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nicholas/Documents/GitHub/peleke/scripts/wandb/run-20250723_065238-tqgxpq4c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://192.168.0.24:8080/nicholas1-santolla/phi4-antibody-epitope/runs/tqgxpq4c' target=\"_blank\">lora-r8-phi-4-9bs</a></strong> to <a href='http://192.168.0.24:8080/nicholas1-santolla/phi4-antibody-epitope' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://192.168.0.24:8080/nicholas1-santolla/phi4-antibody-epitope' target=\"_blank\">http://192.168.0.24:8080/nicholas1-santolla/phi4-antibody-epitope</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://192.168.0.24:8080/nicholas1-santolla/phi4-antibody-epitope/runs/tqgxpq4c' target=\"_blank\">http://192.168.0.24:8080/nicholas1-santolla/phi4-antibody-epitope/runs/tqgxpq4c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Config captured: {'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_bias': 'none', 'lora_task_type': 'TaskType.CAUSAL_LM', 'lora_target_modules': {'qkv_proj', 'o_proj'}, 'lora_fan_in_fan_out': False, 'lora_init_lora_weights': True}\n"
     ]
    }
   ],
   "source": [
    "# Function to extract LoRA config dynamically\n",
    "def get_lora_config_from_model(model):\n",
    "    \"\"\"Extract LoRA configuration from a PEFT model\"\"\"\n",
    "    if hasattr(model, 'peft_config') and model.peft_config:\n",
    "        # Get the first (and usually only) PEFT config\n",
    "        peft_config = list(model.peft_config.values())[0]\n",
    "        \n",
    "        return {\n",
    "            \"lora_r\": peft_config.r,\n",
    "            \"lora_alpha\": peft_config.lora_alpha,\n",
    "            \"lora_dropout\": peft_config.lora_dropout,\n",
    "            \"lora_bias\": peft_config.bias,\n",
    "            \"lora_task_type\": str(peft_config.task_type),\n",
    "            \"lora_target_modules\": peft_config.target_modules,\n",
    "            \"lora_fan_in_fan_out\": getattr(peft_config, 'fan_in_fan_out', False),\n",
    "            \"lora_init_lora_weights\": getattr(peft_config, 'init_lora_weights', True),\n",
    "        }\n",
    "    else:\n",
    "        return {\"lora_config\": \"No PEFT config found\"}\n",
    "\n",
    "# Initialize wandb with dynamic LoRA config\n",
    "wandb.init(\n",
    "    project=\"phi4-antibody-epitope\",\n",
    "    name=f\"lora-r{get_lora_config_from_model(model).get('lora_r', 'unknown')}-{model_name.split('/')[-1]}-{training_args.per_device_train_batch_size}bs\",\n",
    "    config={\n",
    "        # Model Configuration\n",
    "        \"model\": model_name,\n",
    "        \"model_type\": \"phi-4\",\n",
    "        \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        \"trainable_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        \"trainable_percentage\": round(sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()) * 100, 4),\n",
    "        \n",
    "        # Dynamic LoRA Configuration\n",
    "        **get_lora_config_from_model(model),  # Unpack LoRA config\n",
    "        \n",
    "        # Training Configuration\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"effective_batch_size\": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"learning_rate\": getattr(training_args, 'learning_rate', 5e-5),\n",
    "        \"optimizer\": training_args.optim,\n",
    "        \"num_epochs\": training_args.num_train_epochs,\n",
    "        \"warmup_steps\": training_args.warmup_steps,\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "        \"fp16\": training_args.fp16,\n",
    "        \"gradient_checkpointing\": training_args.gradient_checkpointing,\n",
    "        \n",
    "        # Data Configuration\n",
    "        \"max_seq_length\": 1024,\n",
    "        \"dataset_size\": len(tokenized_dataset),\n",
    "        \"epitope_tokens_added\": len(epitope_tokens) if 'epitope_tokens' in locals() else 2,\n",
    "        \"data_format\": \"antigen_epitope_to_antibody\",\n",
    "        \n",
    "        # Hardware\n",
    "        \"gpus_used\": torch.cuda.device_count(),\n",
    "        \"gpu_names\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n",
    "    },\n",
    "    tags=[\"phi4\", \"lora\", \"antibody\", \"epitope\", \"fine-tuning\"],\n",
    "    notes=\"Fine-tuning Phi-4 with LoRA for antibody generation from epitope-highlighted antigens\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class GPUMemoryCallback(TrainerCallback):\n",
    "    def __init__(self, log_every_n_steps=50):\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "    \n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        \"\"\"Log GPU memory on logging steps\"\"\"\n",
    "        if state.global_step % self.log_every_n_steps == 0:\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                cached = torch.cuda.memory_reserved(i) / 1024**3\n",
    "                wandb.log({\n",
    "                    f\"gpu_{i}/memory_allocated_gb\": allocated,\n",
    "                    f\"gpu_{i}/memory_cached_gb\": cached,\n",
    "                }, step=state.global_step)\n",
    "\n",
    "# Verify the config was captured correctly\n",
    "print(\"LoRA Config captured:\", get_lora_config_from_model(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3026c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log custom metrics during training\n",
    "def log_custom_metrics():\n",
    "    wandb.log({\n",
    "        \"epitope_tokens_added\": len(epitope_tokens),\n",
    "        \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        \"trainable_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    })\n",
    "\n",
    "\n",
    "log_custom_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncating train dataset: 100%|██████████| 9529/9529 [00:00<00:00, 207359.58 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='83' max='3177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  83/3177 08:58 < 5:42:59, 0.15 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.513500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up SFTTrainer\n",
    "from trl import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    callbacks=[GPUMemoryCallback(log_every_n_steps=50)],  # Log every 50 steps\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "\n",
    "# Save the trained model\n",
    "trainer.save_model()\n",
    "print(f\"Model saved to {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff22ec",
   "metadata": {},
   "source": [
    "### This script is used to clear vram. For testing purposes only and when you want to clear the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf5eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before: 13.99 GB allocated\n",
      "GPU Memory reserved: 15.27 GB reserved\n",
      "Previous model cleared from memory\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "# Clear any existing models from GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check current GPU memory usage\n",
    "print(f\"GPU Memory before: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB allocated\")\n",
    "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB reserved\")\n",
    "# If you have a model loaded, delete it first\n",
    "try:\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Previous model cleared from memory\")\n",
    "except:\n",
    "    print(\"No previous model to clear\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
