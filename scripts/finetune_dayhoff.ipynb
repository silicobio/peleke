{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41c3951",
   "metadata": {},
   "source": [
    "# Fine-Tuning Microsoft's Dayhoff Model for Antibody Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d877ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create --name dayhoff python=3.10\n",
    "# conda activate dayhoff\n",
    "# pip install dayhoff peft trl\n",
    "# conda install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To avoid \"device-side assert triggered\" RuntimeErrors\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "# os.environ[\"MAMBA_DISABLE_FAST_KERNELS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f125d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config\n",
    "MODEL_ID = \"microsoft/Dayhoff-3b-UR90\"\n",
    "DATA_PATH = \"../data/sabdab/sabdab_training_dataset.csv\"  # CSV with columns: antigen, antibody\n",
    "OUTPUT_DIR = \"models/peleke-dayhoff-3b-ur90_2\"\n",
    "CACHE_DIR = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/colby-h100-01-ci/code/.cache/huggingface/\"\n",
    "\n",
    "MAX_LEN = 2048\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUM = 8\n",
    "EPOCHS = 3\n",
    "LR = 2e-4\n",
    "WARMUP = 0.03\n",
    "USE_QLORA = True\n",
    "SEED = 1337\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer Setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR\n",
    "    )\n",
    "\n",
    "## Add epitope tokens\n",
    "# epitope_tokens = [\"[\", \"]\"]\n",
    "# tokenizer.add_special_tokens({\"additional_special_tokens\": epitope_tokens})\n",
    "# special_tokens_dict = {\"additional_special_tokens\": ['|']}\n",
    "# num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "## Add amino acid tokens (which should all exist) and delimiter\n",
    "# amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "# extra_tokens = amino_acids + [\"|\"]\n",
    "# new_tokens = [t for t in extra_tokens if t not in tokenizer.get_vocab()]\n",
    "# tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# num_added = tokenizer.add_tokens(['|'])\n",
    "\n",
    "\n",
    "bnb_config = None\n",
    "# if USE_QLORA:\n",
    "#     from transformers import BitsAndBytesConfig\n",
    "#     bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#         bnb_4bit_use_double_quant=True,\n",
    "#         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     )\n",
    "\n",
    "## Confirm existing special tokens\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n",
    "# print(\"Num tokens added:\", num_added)\n",
    "# print(\"New tokenizer size:\", len(tokenizer))\n",
    "# print(\"ID for |:\", tokenizer.convert_tokens_to_ids(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc49310",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Setup\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    torch_dtype=\"auto\",\n",
    "    # device_map=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ").cuda()\n",
    "\n",
    "## Resize model to accomodate new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if USE_QLORA:\n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        target_modules=\"all-linear\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config).cuda()\n",
    "\n",
    "model.to(DEVICE)\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Dataset\n",
    "## Format: antigen (with [ ] epitope marks), antibody (heavy|light sequence)\n",
    "\n",
    "## Example row:\n",
    "## antigen,antibody\n",
    "## KVFGRCELAAAM[K][R]HGL[D]...GCRL,EVQLVESGG...|DIQMTQSP...\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=DATA_PATH)\n",
    "\n",
    "## Get all column names\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "\n",
    "## Identify columns to remove (keep 'sentence1', 'sentence2', 'label')\n",
    "columns_to_remove = [col for col in all_columns if col not in ['antibody_fv_seqs', 'highlighted_epitope_seqs']]\n",
    "\n",
    "## Remove the unwanted columns and rename to friendlier column names\n",
    "dataset = dataset.remove_columns(columns_to_remove) \\\n",
    "                 .rename_column(\"antibody_fv_seqs\", \"antibody\") \\\n",
    "                 .rename_column(\"highlighted_epitope_seqs\", \"antigen\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7cf5d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Example Function\n",
    "def build_example(example):\n",
    "    ## Clean the sequences\n",
    "    allowed_chars = \"ACDEFGHIKLMNPQRSTVWY[]@*/#\"\n",
    "\n",
    "    antigen = \"\".join([c for c in example['antigen'] if c in allowed_chars])\n",
    "    antibody = \"\".join([c for c in example['antibody'].replace('|','/') if c in allowed_chars])\n",
    "\n",
    "    ## Format: @ANTIGEN/ANTIBODY*\n",
    "    text = f\"@{antigen}/{antibody}*\"\n",
    "\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    # enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "\n",
    "    ## Mask prompt for loss computation\n",
    "    prompt_ids = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    seq_len = len(enc[\"input_ids\"])\n",
    "    labels = [-100] * seq_len\n",
    "    for i in range(len(prompt_ids), seq_len):\n",
    "        labels[i] = enc[\"input_ids\"][i]\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_example(example):\n",
    "#     allowed_chars = \"ACDEFGHIKLMNPQRSTVWY[]@*/#\"\n",
    "\n",
    "#     antigen = \"\".join([c for c in example['antigen'] if c in allowed_chars])\n",
    "#     antibody = \"\".join([c for c in example['antibody'].replace('|','/') if c in allowed_chars])\n",
    "\n",
    "#     # construct the full sequence with start token, epitope markers, and chain delimiter\n",
    "#     text = f\"@{antigen}/{antibody}*\"\n",
    "\n",
    "#     enc = tokenizer(\n",
    "#         text,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=MAX_LEN\n",
    "#     )\n",
    "\n",
    "#     # causal LM labels = input_ids\n",
    "#     enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "\n",
    "#     # ensure attention_mask exists\n",
    "#     if \"attention_mask\" not in enc:\n",
    "#         enc[\"attention_mask\"] = [1] * len(enc[\"input_ids\"])\n",
    "\n",
    "#     return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9669882c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9523\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Map formatting into tokenized dataset\n",
    "# tokenized = dataset.map(build_example, remove_columns=dataset.column_names)\n",
    "tokenized = dataset.map(build_example, remove_columns=dataset['train'].column_names)\n",
    "# tokenized = tokenized.remove_columns(['token_type_ids'])\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1005717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input id: 33\n",
      "Max label id: 30\n",
      "Vocab size: 36\n"
     ]
    }
   ],
   "source": [
    "test_antibody = \"EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLEWVASIYSYYGSTSYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCAREYHSYWSYSWWPRVGLDYWGQGTLVTVSS|AQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLLIYSASSLYSGVPSRFSGSRSGTDFTLTISSLQPEDFATYYCQQASLTALLTFGQGTKVEIK\"\n",
    "test_antigen = \"KIEEGKLVIWINGDKGYNGLAEVGKKFEKDTGIKVTVEHPDKLEEKFPQVAATGDGPDIIFWAHDRFGGYAQSGLLAEITPDKAFQDKL[Y][P][F]TW[D][A]VRYN[G]KLIAYPIAVEALSLIYNKDLLPNPPKTWEEIPALDKELKAKGKSALMFNLQEPYFTWPLIAADGGYAFK[Y]EN[G][K][Y]DIKDVGVDNAGAKAGLTFLVDLIKNKHMNADTDYSIAEAAFNKGETAMTINGPWAWSNIDTSKVNYGVTVLPTFKGQPSKPF[V]GVLSAGINAASPNKELAKEFLENYLLTDEGLEAVNKDKPLGAVALKSY[E][E]EL[A]KDPR[I][A]AT[M][E]N[A][Q][K][G][E][I]M[P]NIPQMSAFWYAVRTAVINAASGRQTVDEALKDAQTIIELYRQSLEIISRYLREQATGAADTAPMGATSRKALETLRRVGDGVQRNHETAFQGMLRKLDIKNEDDVKSLSRVMIHVFSDGVTNWGRIVTLISFGAFVAKHLKTINQESAIEPLAESITDVLVRTKRDWLVKQRGWDGFVEFF\"\n",
    "\n",
    "example = {\n",
    "    'antigen': test_antigen,\n",
    "    'antibody': test_antibody\n",
    "}\n",
    "\n",
    "ex = build_example(example)\n",
    "print(\"Max input id:\", max(ex[\"input_ids\"]))\n",
    "print(\"Max label id:\", max([i for i in ex[\"labels\"] if i != -100]))\n",
    "print(\"Vocab size:\", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7deb3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# from transformers import default_data_collator\n",
    "# data_collator = default_data_collator\n",
    "\n",
    "# from transformers import DataCollatorForSeq2Seq\n",
    "# data_collator = DataCollatorForSeq2Seq(\n",
    "#     tokenizer=tokenizer,\n",
    "#     model=model,\n",
    "#     label_pad_token_id=-100,\n",
    "#     return_tensors=\"pt\",\n",
    "# )\n",
    "\n",
    "def simple_collator(batch):\n",
    "    input_ids = torch.stack([torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(x[\"attention_mask\"], dtype=torch.long) for x in batch])\n",
    "    labels = torch.stack([torch.tensor(x[\"labels\"], dtype=torch.long) for x in batch])\n",
    "\n",
    "    ## Convert input_ids to embeddings\n",
    "    inputs_embeds = model.get_input_embeddings()(input_ids).to(model.dtype)  # match model dtype\n",
    "\n",
    "    inputs_embeds = inputs_embeds.to(DEVICE)\n",
    "    attention_mask = attention_mask.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"inputs_embeds\": inputs_embeds,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# def sft_collator(batch):\n",
    "#     input_ids = torch.stack([torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch])#.to(model.dtype).to(DEVICE)\n",
    "#     # attention_mask = torch.stack([torch.tensor(x[\"attention_mask\"], dtype=torch.long) for x in batch])\n",
    "#     labels = torch.stack([torch.tensor(x[\"labels\"], dtype=torch.long) for x in batch]).to(model.dtype).to(DEVICE)\n",
    "\n",
    "#     # convert input_ids to embeddings and match model dtype\n",
    "#     inputs_embeds = model.get_input_embeddings()(input_ids).to(model.dtype).to(DEVICE)\n",
    "#     # attention_mask = attention_mask.to(DEVICE)\n",
    "#     # labels = labels.to(DEVICE)\n",
    "\n",
    "#     return {\n",
    "#         \"inputs_embeds\": inputs_embeds,\n",
    "#         # \"attention_mask\": attention_mask,\n",
    "#         \"labels\": labels,\n",
    "#     }\n",
    "\n",
    "\n",
    "def sft_collator(batch):\n",
    "    # Stack CPU tensors\n",
    "    input_ids = torch.stack([torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch])\n",
    "    # attention_mask = torch.stack([torch.tensor(x[\"attention_mask\"], dtype=torch.long) for x in batch])\n",
    "    labels = torch.stack([torch.tensor(x[\"labels\"], dtype=torch.long) for x in batch])\n",
    "\n",
    "    # Convert to embeddings on CPU\n",
    "    inputs_embeds = model.get_input_embeddings()(input_ids).to(model.dtype)\n",
    "\n",
    "    return {\n",
    "        \"inputs_embeds\": inputs_embeds.to_device(DEVICE),\n",
    "        # \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels.to_device(DEVICE),\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3f398504",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    label_names=[\"labels\"],\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    # bf16=torch.cuda.is_available(),\n",
    "    optim=\"paged_adamw_8bit\" if USE_QLORA else \"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    "    seed=int(SEED),\n",
    "    # no_cuda=True ## To Test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4dd8c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncating train dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9523/9523 [00:00<00:00, 69950.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized['train'],\n",
    "#     data_collator=data_collator,\n",
    "#     # data_collator=sft_collator,\n",
    "# )\n",
    "\n",
    "\n",
    "# ## Set up SFTTrainer\n",
    "from trl import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    # data_collator=sft_collator,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    # callbacks=[test_callback],  # Log every 50 steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "51259fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids cuda:0 torch.int64\n",
      "labels cuda:0 torch.int64\n",
      "attention_mask cuda:0 torch.int64\n"
     ]
    }
   ],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "    # print(batch.keys())      # e.g., dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
    "    # print(batch[\"input_ids\"].shape)\n",
    "    for key, tensor in batch.items():\n",
    "        print(key, tensor.device, tensor.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c574af57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch tensor devices ===\n",
      "input_ids: device=cuda:0, dtype=torch.int64, shape=torch.Size([2, 1024])\n",
      "labels: device=cuda:0, dtype=torch.int64, shape=torch.Size([2, 1024])\n",
      "attention_mask: device=cuda:0, dtype=torch.int64, shape=torch.Size([2, 1024])\n",
      "\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Get a single batch from your trainer\n",
    "batch = next(iter(trainer.get_train_dataloader()))\n",
    "\n",
    "print(\"=== Batch tensor devices ===\")\n",
    "for key, tensor in batch.items():\n",
    "    print(f\"{key}: device={tensor.device}, dtype={tensor.dtype}, shape={tensor.shape}\")\n",
    "\n",
    "# Check model device\n",
    "model_device = next(model.parameters()).device\n",
    "print(f\"\\nModel device: {model_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e872ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8fcb7539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the trained model\n",
    "trainer.save_model(training_args.output_dir)\n",
    "print(f\"Model saved to {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ece34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference Test\n",
    "def generate_antibody(antigen: str, max_new_tokens: int = 700):\n",
    "    prompt = tokenizer.bos_token + antigen + tokenizer.sep_token\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    text = tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n",
    "    return text.split(tokenizer.sep_token, 1)[-1].rstrip(tokenizer.eos_token)\n",
    "\n",
    "\n",
    "test_antigen = \"KVFGRCELAAAM[K][R]HGL[D][N][Y]RG[Y][S]LG[N]WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA[K]KIVSDGNGMNAWVAWRNRCK[G][T][D]V[Q]AW[I][R]GCRL\"\n",
    "print(\"Generated antibody:\", generate_antibody(test_antigen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dayhoff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
