{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3deb958f",
   "metadata": {},
   "source": [
    "# Fine-Tune an LLM for Antibody Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920184da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f381adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os, re\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Test your GPU setup\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3861480",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset\n",
    "df = pd.read_csv(\"../data/sabdab/sabdab_training_dataset.csv\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8023680",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove rows with missing sequences\n",
    "df = df.dropna(subset=['h_chain_seq', 'l_chain_seq', 'antigen_seqs', 'highlighted_epitope_seqs'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17677f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load base tokenizer and model FIRST\n",
    "model_name = \"microsoft/phi-4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16, # Load model in bfloat16 for better performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add epitope tokens\n",
    "epitope_tokens = [\"<epi>\", \"</epi>\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": epitope_tokens})\n",
    "\n",
    "## Add amino acid tokens\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "extra_tokens = amino_acids + [\"|\"]\n",
    "new_tokens = [t for t in extra_tokens if t not in tokenizer.get_vocab()]\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "## Add task-specific tokens\n",
    "task_tokens = [\"Antigen\", \"Antibody\"]\n",
    "tokenizer.add_tokens(task_tokens)\n",
    "\n",
    "## Resize model embeddings ONCE after adding all tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Epitope and Prompt Formatter function\n",
    "def format_prompt(example):\n",
    "    epitope_seq = re.sub(r'\\[([A-Z])\\]', r'<epi>\\1</epi>', example['highlighted_epitope_seqs'])\n",
    "    return {\n",
    "        \"text\": f\"Antigen: {epitope_seq}<|im_end|>\\nAntibody: {example['antibody_fv_seqs']}<|im_end|>\\n\"\n",
    "    }\n",
    "\n",
    "## Create dataset with all tokens available\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check truncation at 800\n",
    "sequence_lengths = [len(tokenizer(example[\"text\"], truncation=False)[\"input_ids\"]) for example in dataset]\n",
    "truncated_800 = sum(1 for length in sequence_lengths if length > 800)\n",
    "print(f\"Sequences truncated at max_length=800: {truncated_800}/{len(sequence_lengths)} ({100*truncated_800/len(sequence_lengths):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the dataset\n",
    "def tokenize(example):\n",
    "    encoded = tokenizer(example[\"text\"], truncation=True, max_length=800)\n",
    "    # Make sure labels are a proper list, not nested\n",
    "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    return encoded\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ecba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tokenization is working with epitope tokens\n",
    "print(\"Sample tokenized text:\")\n",
    "sample_tokens = tokenizer.tokenize(dataset[0]['text'][:200])\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb594211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\n",
    "    'pdb_id', 'h_chain_id', 'l_chain_id', 'antigen_ids', 'antigen_seqs',\n",
    "    'h_chain_seq', 'l_chain_seq', 'antibody_seqs',\n",
    "    'highlighted_epitope_seqs', 'epitope_residues','h_chain_fv_seq',\n",
    "       'l_chain_fv_seq', 'antibody_fv_seqs', 'text'\n",
    "])\n",
    "print(\"Columns after removal:\", tokenized_dataset.column_names)\n",
    "# Should show: ['input_ids', 'attention_mask', 'labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the gradient fix to your model\n",
    "if hasattr(model, 'enable_input_require_grads'):\n",
    "    model.enable_input_require_grads()\n",
    "else:\n",
    "    def make_inputs_require_grad(module, input, output):\n",
    "        output.requires_grad_(True)\n",
    "    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eecbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data collator\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=False,\n",
    "#     return_tensors=\"pt\",\n",
    "#     pad_to_multiple_of=8, # Pad to multiple of 8 for better performance on GPUs\n",
    "# )\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12048eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Configure LoRA\n",
    "## PEFT configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    # target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    target_modules=[\"o_proj\", \"qkv_proj\"],\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    # output_dir=f\"../models/peleke-{model_name.split('/')[-1]}-0806025\",\n",
    "    output_dir=f\"../models/peleke-phi-4-h100-20250810-10eps\",\n",
    "    per_device_train_batch_size=9,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=10,\n",
    "    warmup_steps=25,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    logging_dir=\"../logs\",\n",
    "    logging_steps=25,\n",
    "    gradient_checkpointing=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\", #\"wandb\",  # Enable wandb reporting\n",
    "    run_name=f\"lora-epitope-{model_name.split('/')[-1]}\",  # Run name for wandb\n",
    "    # optim=\"adamw_torch\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    dataloader_num_workers=8,  # Add parallel data loading\n",
    "    dataloader_pin_memory=True,  # Pin memory for faster data loading\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04199b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_brackets_to_epi(sequence):\n",
    "    \"\"\"Convert [X] format to <epi>X</epi> format\"\"\"\n",
    "    return re.sub(r'\\[([A-Z])\\]', r'<epi>\\1</epi>', sequence)\n",
    "\n",
    "# Convert your bracket sequences to the training format\n",
    "sequences_with_brackets = [\n",
    "    \"KVFGRCELAAAM[K][R]HGL[D][N][Y]RG[Y][S]LG[N]WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA[K]KIVSDGNGMNAWVAWRNRCK[G][T][D]V[Q]AW[I][R]GCRL\",\n",
    "    \"NLCPFHEVFNATTFASVYAWNRKRISNCVADYSVIYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVI[R]G[N]EV[S][Q]IAPGQ[T]GNIADYNYKLPDDFTGCVIAWNSN[K]LDSKPSGNYNYLYRLLRKSKLKPFERDISTEIYQAGNKPCNGVAGPNCYSPLQSYGF[R]P[T][Y][G][V]GH[Q]PYRVVVLSFELLHAPATVCGP\",\n",
    "]\n",
    "\n",
    "# Convert to the exact training format\n",
    "test_antigens = [convert_brackets_to_epi(seq) for seq in sequences_with_brackets]\n",
    "\n",
    "# Verify the conversion\n",
    "for i, (orig, conv) in enumerate(zip(sequences_with_brackets, test_antigens)):\n",
    "    print(f\"=== Sequence {i+1} ===\")\n",
    "    print(f\"Original: {orig[:60]}...\")\n",
    "    print(f\"Converted: {conv[:60]}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nFinal test_antigens for training callback:\")\n",
    "for i, antigen in enumerate(test_antigens):\n",
    "    print(f\"Test {i+1}: {antigen[:80]}...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class TestGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, model, tokenizer, test_antigens, log_every_n_steps=100, output_file=\"test_generations.txt\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_antigens = test_antigens\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "        self.output_file = output_file\n",
    "        \n",
    "        # Create/clear the output file\n",
    "        with open(self.output_file, 'w') as f:\n",
    "            f.write(f\"Test Generation Log - Started: {datetime.now()}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    def create_test_prompt(self, antigen_with_epitopes):\n",
    "        return f\"Antigen: {antigen_with_epitopes}<|im_end|>\\nAntibody:\"\n",
    "    \n",
    "    def generate_antibody_test(self, antigen_with_epitopes, max_length=800):\n",
    "        \"\"\"Generate antibody for testing during training\"\"\"\n",
    "        prompt = self.create_test_prompt(antigen_with_epitopes)\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        self.model.eval()\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=200,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "                    repetition_penalty=1.1,\n",
    "                )\n",
    "                \n",
    "                # Decode and extract antibody\n",
    "                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "                if \"Antibody:\" in generated_text:\n",
    "                    antibody_part = generated_text.split(\"Antibody:\", 1)[1]\n",
    "                    if \"<|im_end|>\" in antibody_part:\n",
    "                        antibody_sequence = antibody_part.split(\"<|im_end|>\", 1)[0].strip()\n",
    "                    else:\n",
    "                        antibody_sequence = antibody_part.strip()\n",
    "                else:\n",
    "                    antibody_sequence = \"Generation failed\"\n",
    "                \n",
    "                return antibody_sequence\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "        finally:\n",
    "            self.model.train()  # Put model back in training mode\n",
    "    \n",
    "    def run_test_generation(self, state, phase=\"TRAINING\"):\n",
    "        \"\"\"Run test generation and print/save results\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        header = f\"TEST GENERATION - {phase} - STEP {state.global_step} - {timestamp}\"\n",
    "        \n",
    "        # Print to terminal\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(header)\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Write to file\n",
    "        with open(self.output_file, 'a') as f:\n",
    "            f.write(f\"\\n{'='*80}\\n\")\n",
    "            f.write(f\"{header}\\n\")\n",
    "            f.write(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for i, test_antigen in enumerate(self.test_antigens):\n",
    "            case_header = f\"--- Test Case {i+1} ---\"\n",
    "            input_display = f\"Input: {test_antigen[:60]}{'...' if len(test_antigen) > 60 else ''}\"\n",
    "            \n",
    "            # Generate antibody\n",
    "            antibody = self.generate_antibody_test(test_antigen)\n",
    "            generated_display = f\"Generated: {antibody}\"\n",
    "            \n",
    "            # Print to terminal\n",
    "            print(f\"\\n{case_header}\")\n",
    "            print(input_display)\n",
    "            print(generated_display)\n",
    "            \n",
    "            # Write to file (with full input)\n",
    "            with open(self.output_file, 'a') as f:\n",
    "                f.write(f\"\\n{case_header}\\n\")\n",
    "                f.write(f\"Full Input: {test_antigen}\\n\")\n",
    "                f.write(f\"Generated: {antibody}\\n\")\n",
    "                f.write(f\"Length: {len(antibody)} characters\\n\")\n",
    "        \n",
    "        # Terminal footer\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # File footer\n",
    "        with open(self.output_file, 'a') as f:\n",
    "            f.write(f\"{'='*80}\\n\\n\")\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Test at the beginning of training\"\"\"\n",
    "        print(\"ðŸ§¬ INITIAL GENERATION TEST (Before Training)\")\n",
    "        self.run_test_generation(state, \"INITIAL\")\n",
    "    \n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        \"\"\"Test periodically during training\"\"\"\n",
    "        if state.global_step % self.log_every_n_steps == 0 and state.global_step > 0:\n",
    "            self.run_test_generation(state, \"PERIODIC\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Test at the end of training\"\"\"\n",
    "        print(\"ðŸŽ‰ FINAL GENERATION TEST (After Training)\")\n",
    "        self.run_test_generation(state, \"FINAL\")\n",
    "        \n",
    "        # Add summary to file\n",
    "        with open(self.output_file, 'a') as f:\n",
    "            f.write(f\"\\nTraining completed: {datetime.now()}\\n\")\n",
    "            f.write(f\"Final step: {state.global_step}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc859af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the callback\n",
    "test_callback = TestGenerationCallback(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    test_antigens=test_antigens,\n",
    "    log_every_n_steps=50,  ## Test every 50 steps\n",
    "    output_file=\"../logs/test_generations.txt\"  ## Save to logs directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save tokenizer\n",
    "tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up SFTTrainer\n",
    "from trl import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    callbacks=[test_callback],  # Log every 50 steps\n",
    ")\n",
    "\n",
    "## Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb run\n",
    "#wandb.finish()\n",
    "\n",
    "## Save the trained model\n",
    "trainer.save_model(training_args.output_dir)\n",
    "print(f\"Model saved to {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852cfebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87484d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c239d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07ff22ec",
   "metadata": {},
   "source": [
    "### This script is used to clear vram. For testing purposes only and when you want to clear the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "# Clear any existing models from GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check current GPU memory usage\n",
    "print(f\"GPU Memory before: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB allocated\")\n",
    "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB reserved\")\n",
    "# If you have a model loaded, delete it first\n",
    "try:\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Previous model cleared from memory\")\n",
    "except:\n",
    "    print(\"No previous model to clear\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
