{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5021b187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ -----------------------\n",
      "accelerate               1.8.1\n",
      "aiohappyeyeballs         2.6.1\n",
      "aiohttp                  3.12.13\n",
      "aiosignal                1.3.2\n",
      "annotated-types          0.7.0\n",
      "asttokens                3.0.0\n",
      "attrs                    25.3.0\n",
      "certifi                  2025.6.15\n",
      "charset-normalizer       3.4.2\n",
      "click                    8.2.1\n",
      "comm                     0.2.2\n",
      "datasets                 3.6.0\n",
      "debugpy                  1.8.14\n",
      "decorator                5.2.1\n",
      "dill                     0.3.8\n",
      "executing                2.2.0\n",
      "filelock                 3.13.1\n",
      "frozenlist               1.7.0\n",
      "fsspec                   2024.6.1\n",
      "gitdb                    4.0.12\n",
      "GitPython                3.1.44\n",
      "hf-xet                   1.1.5\n",
      "huggingface-hub          0.33.2\n",
      "idna                     3.10\n",
      "ipykernel                6.29.5\n",
      "ipython                  9.4.0\n",
      "ipython_pygments_lexers  1.1.1\n",
      "jedi                     0.19.2\n",
      "Jinja2                   3.1.4\n",
      "jupyter_client           8.6.3\n",
      "jupyter_core             5.8.1\n",
      "MarkupSafe               2.1.5\n",
      "matplotlib-inline        0.1.7\n",
      "mpmath                   1.3.0\n",
      "multidict                6.6.3\n",
      "multiprocess             0.70.16\n",
      "nest-asyncio             1.6.0\n",
      "networkx                 3.3\n",
      "numpy                    2.1.2\n",
      "nvidia-cublas-cu12       12.4.5.8\n",
      "nvidia-cuda-cupti-cu12   12.4.127\n",
      "nvidia-cuda-nvrtc-cu12   12.4.127\n",
      "nvidia-cuda-runtime-cu12 12.4.127\n",
      "nvidia-cudnn-cu12        9.1.0.70\n",
      "nvidia-cufft-cu12        11.2.1.3\n",
      "nvidia-curand-cu12       10.3.5.147\n",
      "nvidia-cusolver-cu12     11.6.1.9\n",
      "nvidia-cusparse-cu12     12.3.1.170\n",
      "nvidia-cusparselt-cu12   0.6.2\n",
      "nvidia-nccl-cu12         2.25.1\n",
      "nvidia-nvjitlink-cu12    12.4.127\n",
      "nvidia-nvtx-cu12         12.4.127\n",
      "packaging                25.0\n",
      "pandas                   2.3.0\n",
      "parso                    0.8.4\n",
      "peft                     0.15.2\n",
      "pexpect                  4.9.0\n",
      "pillow                   11.0.0\n",
      "pip                      25.1.1\n",
      "platformdirs             4.3.8\n",
      "prompt_toolkit           3.0.51\n",
      "propcache                0.3.2\n",
      "protobuf                 6.31.1\n",
      "psutil                   7.0.0\n",
      "ptyprocess               0.7.0\n",
      "pure_eval                0.2.3\n",
      "pyarrow                  20.0.0\n",
      "pydantic                 2.11.7\n",
      "pydantic_core            2.33.2\n",
      "Pygments                 2.19.2\n",
      "python-dateutil          2.9.0.post0\n",
      "pytorch-triton           3.2.0+git4b3bb1f8\n",
      "pytz                     2025.2\n",
      "PyYAML                   6.0.2\n",
      "pyzmq                    27.0.0\n",
      "regex                    2024.11.6\n",
      "requests                 2.32.4\n",
      "safetensors              0.5.3\n",
      "sentry-sdk               2.32.0\n",
      "setuptools               59.6.0\n",
      "six                      1.17.0\n",
      "smmap                    5.0.2\n",
      "stack-data               0.6.3\n",
      "sympy                    1.14.0\n",
      "tokenizers               0.21.2\n",
      "torch                    2.7.0.dev20250310+cu124\n",
      "tornado                  6.5.1\n",
      "tqdm                     4.67.1\n",
      "traitlets                5.14.3\n",
      "transformers             4.53.0\n",
      "typing_extensions        4.14.0\n",
      "typing-inspection        0.4.1\n",
      "tzdata                   2025.2\n",
      "urllib3                  2.5.0\n",
      "wandb                    0.21.0\n",
      "wcwidth                  0.2.13\n",
      "xxhash                   3.5.0\n",
      "yarl                     1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc68948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75505853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 1\n",
      "GPU: NVIDIA GeForce RTX 3090 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 191.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 3,822,652,416 || trainable%: 0.0411\n",
      "PEFT model created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # MUST be first line\n",
    "\n",
    "# Now import everything else\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Verify only 3090 Ti is visible\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")  # Should show 3090 Ti\n",
    "\n",
    "# Load model on the isolated 3090 Ti\n",
    "model_name = model_name = \"microsoft/Phi-3.5-mini-instruct\" #\"microsoft/phi-4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# PEFT configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], #[\"o_proj\", \"qkv_proj\"],\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "# Print trainable parameters to verify\n",
    "peft_model.print_trainable_parameters()\n",
    "print(\"PEFT model created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd54e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, DataCollatorForLanguageModeling, Trainer# AutoTokenizer, AutoModelForCausalLM, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d63ac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb</th>\n",
       "      <th>Hchain</th>\n",
       "      <th>Lchain</th>\n",
       "      <th>AntigenChains</th>\n",
       "      <th>HeavySeq</th>\n",
       "      <th>LightSeq</th>\n",
       "      <th>AntigenSeq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8xa4</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>A | B</td>\n",
       "      <td>QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERVTLSCRASQRVSSTYLAWYQQKPGQAPR...</td>\n",
       "      <td>SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9cph</td>\n",
       "      <td>H</td>\n",
       "      <td>L</td>\n",
       "      <td>A</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...</td>\n",
       "      <td>AQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLL...</td>\n",
       "      <td>KIEEGKLVIWINGDKGYNGLAEVGKKFEKDTGIKVTVEHPDKLEEK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9d7i</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "      <td>E</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9d7i</td>\n",
       "      <td>J</td>\n",
       "      <td>I</td>\n",
       "      <td>C</td>\n",
       "      <td>VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9d7o</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "      <td>E</td>\n",
       "      <td>QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...</td>\n",
       "      <td>YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...</td>\n",
       "      <td>LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pdb Hchain Lchain AntigenChains  \\\n",
       "4   8xa4      C      D         A | B   \n",
       "9   9cph      H      L             A   \n",
       "10  9d7i      H      G             E   \n",
       "11  9d7i      J      I             C   \n",
       "12  9d7o      H      G             E   \n",
       "\n",
       "                                             HeavySeq  \\\n",
       "4   QLQLQESGPGLVKPSETLSLTCTVSGGSISSNNDYWGWIRQPPGKG...   \n",
       "9   EVQLVESGGGLVQPGGSLRLSCAASGFNLSSSSIHWVRQAPGKGLE...   \n",
       "10  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "11  VQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLEW...   \n",
       "12  QVQLQESGPGVVKSSETLSLTCTVSGGSMGGTYWSWLRLSPGKGLE...   \n",
       "\n",
       "                                             LightSeq  \\\n",
       "4   EIVLTQSPGTLSLSPGERVTLSCRASQRVSSTYLAWYQQKPGQAPR...   \n",
       "9   AQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLL...   \n",
       "10  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "11  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "12  YELTQPPSVSVSPGQTATITCSGASTNVCWYQVKPGQSPEVVIFEN...   \n",
       "\n",
       "                                           AntigenSeq  \n",
       "4   SCNGLYYQGSCYILHSDYKSFEDAKANCAAESSTLPNKSDVLTTWL...  \n",
       "9   KIEEGKLVIWINGDKGYNGLAEVGKKFEKDTGIKVTVEHPDKLEEK...  \n",
       "10  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...  \n",
       "11  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...  \n",
       "12  LWVTVYYGVPVWKDAETTLFCASDNVWATHACVPTDPNPQEIHLEN...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load dataset\n",
    "df = pd.read_csv(\"./data/sabdab/sabdab_with_sequences.tsv\", sep='\\t')\n",
    "\n",
    "## Remove rows with missing sequences\n",
    "df = df.dropna(subset=['HeavySeq', 'LightSeq', 'AntigenSeq'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e7823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10073/10073 [00:00<00:00, 35457.34 examples/s]\n",
      "Map: 100%|██████████| 10073/10073 [00:02<00:00, 3649.97 examples/s]\n",
      "/tmp/ipykernel_1766304/3085847355.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "## Format prompts\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"Antigen: {example['AntigenSeq']}\\nAntibody: {example['HeavySeq']}|{example['LightSeq']}\\n\"\n",
    "    }\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "## Extend tokenizer with special tokens\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "extra_tokens = amino_acids + [\"|\"]# [\"[\", \"]\", \"|\"]\n",
    "\n",
    "## Check if tokens already exist in the tokenizer's vocabulary\n",
    "new_tokens = [t for t in extra_tokens if t not in tokenizer.get_vocab()]\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.train()\n",
    "\n",
    "## Tokenize the dataset\n",
    "def tokenize(example):\n",
    "    # return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    encoded = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # encoded[\"labels\"] = encoded[\"input_ids\"]#.copy()\n",
    "    return encoded\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)\n",
    "\n",
    "## Remove unnecessary columns from the tokenized dataset\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\n",
    "    'pdb', 'Hchain', 'Lchain', 'AntigenSeq', 'AntigenChains',\n",
    "    'HeavySeq', 'LightSeq', '__index_level_0__', 'text'\n",
    "])\n",
    "\n",
    "## Split the dataset into train and validation sets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=1337)\n",
    "\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "train_dataset\n",
    "\n",
    "## Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../models/peleke-{model_name.split('/')[-1]}\",\n",
    "    ## Batching\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    ## Epochs and warmups\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=100,\n",
    "    ## Optimization\n",
    "    weight_decay=0.01,\n",
    "    ## Logging and saving\n",
    "    logging_dir=\"../logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    # fp16=True,\n",
    "    gradient_checkpointing=True, ## If having memory issues\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "## Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  ## Important: MLM=False for causal LM\n",
    ")\n",
    "\n",
    "## Trainer\n",
    "trainer = Trainer(\n",
    "    # model=model,\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9408ed15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/nicholas/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training mode: True\n",
      "Trainable params: 1572864\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel training mode:\u001b[39m\u001b[33m\"\u001b[39m, peft_model.training)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrainable params:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(p.numel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m peft_model.parameters() \u001b[38;5;28;01mif\u001b[39;00m p.requires_grad))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/trainer.py:2207\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2205\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/trainer.py:2549\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2542\u001b[39m context = (\n\u001b[32m   2543\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2544\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2546\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2547\u001b[39m )\n\u001b[32m   2548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2552\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2554\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2555\u001b[39m ):\n\u001b[32m   2556\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2557\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/transformers/trainer.py:3798\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3796\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3798\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3800\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:2553\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2553\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Check model is ready for training\n",
    "print(\"Model training mode:\", peft_model.training)\n",
    "print(\"Trainable params:\", sum(p.numel() for p in peft_model.parameters() if p.requires_grad))\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6a37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d107bd1",
   "metadata": {},
   "source": [
    "#This code worked with the following versions:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cd5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 144.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory after loading: 7.64 GB\n",
      "=== PEFT MODEL INFO ===\n",
      "trainable params: 197,787,648 || all params: 3,821,865,984 || trainable%: 5.1752\n",
      "Memory after PEFT: 7.65 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10073/10073 [00:00<00:00, 36158.85 examples/s]\n",
      "Map: 100%|██████████| 10073/10073 [00:03<00:00, 2643.80 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before trainer: 7.65 GB\n",
      "Memory after trainer: 7.65 GB\n",
      "Memory before training: 7.65 GB\n",
      "Memory available: 17.65 GB\n",
      "\n",
      "=== STARTING ULTRA MEMORY-OPTIMIZED TRAINING ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 16:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.169900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training completed successfully!\n",
      "Saving model...\n",
      "✓ Model saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add new tokens\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "extra_tokens = amino_acids + [\"|\"]\n",
    "new_tokens = [t for t in extra_tokens if t not in tokenizer.get_vocab()]\n",
    "if new_tokens:\n",
    "    print(f\"Adding {len(new_tokens)} new tokens: {new_tokens}\")\n",
    "    tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Load model with memory optimizations (NO flash attention)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 to save memory\n",
    "    low_cpu_mem_usage=True,\n",
    "    # Removed attn_implementation=\"flash_attention_2\"\n",
    ").cuda()\n",
    "\n",
    "# Resize embeddings\n",
    "if new_tokens:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Resized embeddings to {len(tokenizer)} tokens\")\n",
    "\n",
    "print(f\"Model memory after loading: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "\n",
    "# PEFT configuration - even smaller to save memory\n",
    "peft_config = LoraConfig(\n",
    "    r=4,  # Very small rank\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"o_proj\"],  # Only target one module type\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.train()\n",
    "\n",
    "# Enable gradients for embeddings\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'embed_tokens' in name or 'lm_head' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(\"=== PEFT MODEL INFO ===\")\n",
    "peft_model.print_trainable_parameters()\n",
    "print(f\"Memory after PEFT: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "\n",
    "# Dataset processing with very short sequences\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"Antigen: {example['AntigenSeq']}\\nAntibody: {example['HeavySeq']}|{example['LightSeq']}\\n\"\n",
    "    }\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "def tokenize(example):\n",
    "    encoded = tokenizer(\n",
    "        example[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=64  # Very short sequences\n",
    "    )\n",
    "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    return encoded\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\n",
    "    'pdb', 'Hchain', 'Lchain', 'AntigenSeq', 'AntigenChains',\n",
    "    'HeavySeq', 'LightSeq', '__index_level_0__', 'text'\n",
    "])\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=1337)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Ultra memory-optimized training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../models/peleke-{model_name.split('/')[-1]}\",\n",
    "    per_device_train_batch_size=150,  # Minimal batch size\n",
    "    per_device_eval_batch_size=150,\n",
    "    num_train_epochs=5,  # Start with 1 epoch for testing\n",
    "    gradient_accumulation_steps=8,  # Simulate batch size of 8\n",
    "   # max_steps=200,  # Start with just 200 steps for testing\n",
    "    warmup_steps=20,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"../logs\",\n",
    "    logging_steps=20,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    bf16=True,  # Mixed precision\n",
    "    gradient_checkpointing=True,  # Trade compute for memory\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    save_strategy=\"epoch\",  # Save model every epoch\n",
    "    # Additional memory optimizations\n",
    "    save_total_limit=1,  # Only keep 1 checkpoint\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "print(f\"Memory before trainer: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(f\"Memory after trainer: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "\n",
    "# Final memory cleanup before training\n",
    "del model  # Remove reference to base model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Memory before training: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "print(f\"Memory available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\n=== STARTING ULTRA MEMORY-OPTIMIZED TRAINING ===\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"✓ Training completed successfully!\")\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    peft_model.save_pretrained(\"./phi35-antibody-lora-e10\")\n",
    "    tokenizer.save_pretrained(\"./phi35-antibody-lora-e10\")\n",
    "    print(\"✓ Model saved!\")\n",
    "    \n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(f\"Still OOM: {e}\")\n",
    "    print(\"Current GPU memory usage:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    print(\"Try using an even smaller model like TinyLlama-1.1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d194dea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 201.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antigen: MKFLVNVALVFMVVYISYIYA\n",
      "Antibody: Matching: \n",
      "\n",
      "Step 1: Identify the complementary base pairs in the sequence. \n",
      "\n",
      "Step 2: Create a string of matches, where each match consists of a pair of complementary bases.\n",
      "\n",
      "Step 3: Count the number of matches found in the sequence.\n",
      "\n",
      "Step 4: If the number of matches is equal to the length of the antigen sequence, then a perfect match has been found.\n",
      "\n",
      "Based on the\n",
      "--------------------------------------------------\n",
      "Antigen: ACDEFGHIKLMNPQRSTVWY\n",
      "Antibody: Amino Acid: A\n",
      "Antibody: T\n",
      "\n",
      "Amino Acid: C\n",
      "Antibody: G\n",
      "\n",
      "Amino Acid: D\n",
      "Antibody: T\n",
      "\n",
      "Amino Acid: E\n",
      "Antibody: Y\n",
      "\n",
      "Amino Acid: F\n",
      "Antibody: T\n",
      "\n",
      "Amino Acid: G\n",
      "Antibody: T\n",
      "\n",
      "Amino Acid: H\n",
      "--------------------------------------------------\n",
      "Antigen: YYWGQGTLVTVSS\n",
      "Antibody: 5K5S9.7\n",
      "\n",
      "Please provide a detailed analysis of the potential implications of this interaction in the context of immunology. Discuss the possible functions and effects of this antigen-antibody pairing, including any potential impacts on cellular processes, signaling pathways, or immune responses. Consider the properties of the antigen and antibody, as well as any known factors that may influence their interaction. Support your discussion with relevant scientific evidence and\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load model\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./phi35-antibody-lora\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\n",
    "model = PeftModel.from_pretrained(base_model, \"./phi35-antibody-lora\").cuda()\n",
    "model.eval()\n",
    "\n",
    "# Generate complete antibody sequences\n",
    "test_antigens = [\n",
    "    \"MKFLVNVALVFMVVYISYIYA\",\n",
    "    \"ACDEFGHIKLMNPQRSTVWY\", \n",
    "    \"YYWGQGTLVTVSS\"\n",
    "]\n",
    "\n",
    "for antigen in test_antigens:\n",
    "    prompt = f\"Antigen: {antigen}\\nAntibody: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=False,\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    antibody_sequence = full_text.replace(prompt, \"\").strip()\n",
    "    \n",
    "    print(f\"Antigen: {antigen}\")\n",
    "    print(f\"Antibody: {antibody_sequence}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db05e77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING YOUR NEWLY TRAINED MODEL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 192.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully!\n",
      "Prompt: 'Antigen: MKFLVNVALVFMVVYISYIYA\n",
      "Antibody: '\n",
      "Top 10 predictions:\n",
      "   1. '\n",
      "' (score: 49.00)\n",
      "   2. 'Y' (score: 47.00)\n",
      "   3. 'F' (score: 46.25)\n",
      "   4. 'V' (score: 46.25)\n",
      "   5. 'D' (score: 46.25)\n",
      "   6. 'M' (score: 46.25)\n",
      "   7. 'Q' (score: 46.00)\n",
      "   8. '1' (score: 46.00)\n",
      "   9. 'K' (score: 45.75)\n",
      "  10. 'I' (score: 45.50)\n",
      "\n",
      "Prompt: 'Antigen: ACDEFGHIKLMNPQRSTVWY\n",
      "Antibody: H'\n",
      "Top 10 predictions:\n",
      "   1. 'I' (score: 47.75)\n",
      "   2. 'G' (score: 47.50)\n",
      "   3. 'IE' (score: 47.50)\n",
      "   4. 'Y' (score: 47.00)\n",
      "   5. 'J' (score: 47.00)\n",
      "   6. 'IG' (score: 46.75)\n",
      "   7. 'HH' (score: 46.50)\n",
      "   8. 'K' (score: 46.25)\n",
      "   9. 'L' (score: 46.25)\n",
      "  10. 'IL' (score: 46.25)\n",
      "\n",
      "Prompt: 'Antigen: TEST\n",
      "Antibody: HEAVY|'\n",
      "Top 10 predictions:\n",
      "   1. 'LI' (score: 51.25)\n",
      "   2. 'NE' (score: 50.25)\n",
      "   3. 'M' (score: 50.25)\n",
      "   4. 'B' (score: 50.25)\n",
      "   5. 'L' (score: 50.00)\n",
      "   6. 'MA' (score: 50.00)\n",
      "   7. '\n",
      "' (score: 50.00)\n",
      "   8. 'LY' (score: 49.50)\n",
      "   9. 'CH' (score: 49.50)\n",
      "  10. 'ME' (score: 49.50)\n",
      "\n",
      "=== IMPROVEMENT CHECK ===\n",
      "Your model should now:\n",
      "1. Predict amino acids after 'Antigen:'\n",
      "2. Predict amino acid sequences after 'Antibody:'\n",
      "3. Understand the Heavy|Light chain format\n",
      "4. Show lower loss than the initial 4.17\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TESTING YOUR NEWLY TRAINED MODEL ===\")\n",
    "\n",
    "# Load your trained model\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./phi35-antibody-lora-e10\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ").cuda()\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"./phi35-antibody-lora-e10\").cuda()\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "\n",
    "# Test what the model learned\n",
    "def test_antibody_understanding(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        top_k = torch.topk(logits, 10)\n",
    "        \n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print(\"Top 10 predictions:\")\n",
    "        for i, (score, token_id) in enumerate(zip(top_k.values, top_k.indices)):\n",
    "            token = tokenizer.decode(token_id.item())\n",
    "            print(f\"  {i+1:2d}. '{token}' (score: {score.item():.2f})\")\n",
    "        print()\n",
    "\n",
    "# Test the model's antibody knowledge\n",
    "test_prompts = [\n",
    "    \"Antigen: MKFLVNVALVFMVVYISYIYA\\nAntibody: \",\n",
    "    \"Antigen: ACDEFGHIKLMNPQRSTVWY\\nAntibody: H\",\n",
    "    \"Antigen: TEST\\nAntibody: HEAVY|\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    test_antibody_understanding(prompt)\n",
    "\n",
    "# Compare with original predictions to see improvement\n",
    "print(\"=== IMPROVEMENT CHECK ===\")\n",
    "print(\"Your model should now:\")\n",
    "print(\"1. Predict amino acids after 'Antigen:'\")\n",
    "print(\"2. Predict amino acid sequences after 'Antibody:'\") \n",
    "print(\"3. Understand the Heavy|Light chain format\")\n",
    "print(\"4. Show lower loss than the initial 4.17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c5024",
   "metadata": {},
   "source": [
    "This was testing the finetuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50080367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 183.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING ANTIBODY GENERATION ===\n"
     ]
    }
   ],
   "source": [
    "# Load your trained model\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the base model and your fine-tuned adapter\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./phi35-antibody-lora\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ").cuda()\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model = PeftModel.from_pretrained(base_model, \"./phi35-antibody-lora\").cuda()\n",
    "model.eval()\n",
    "\n",
    "print(\"=== TESTING ANTIBODY GENERATION ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ff3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f955b36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Antigen: MKFLVNVALVFMVVYISYIYA\n",
      "Generated Antibody: The alignment score is calculated as follows:\n",
      "\n",
      "Score = (Number of matches) * 1 + (Number of mismatches) * -1\n",
      "\n",
      "Number of matches: 12\n",
      "Number of mismatches: 6\n",
      "--------------------------------------------------\n",
      "\n",
      "Antigen: ATGCDEFGHIKLMNPQRSTVWY\n",
      "Generated Antibody: 1. N-terminal\n",
      "2. C-terminal\n",
      "3. Internal\n",
      "\n",
      "\n",
      "# Response\n",
      "\n",
      "The antibody can bind to the antigen at any of the three given locations: N-terminal, C\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Give it an antigen, see if it generates reasonable antibody sequences\n",
    "test_antigens = [\n",
    "    \"MKFLVNVALVFMVVYISYIYA\",  # Example antigen sequence\n",
    "    \"ATGCDEFGHIKLMNPQRSTVWY\",  # Another test sequence\n",
    "]\n",
    "\n",
    "for antigen in test_antigens:\n",
    "    prompt = f\"Antigen: {antigen}\\nAntibody: \"\n",
    "    \n",
    "    # FIXED: Move tensors to GPU properly\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,  # Reduced for testing\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=False,\n",
    "            )\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generated_antibody = generated_text.replace(prompt, \"\").strip()\n",
    "            \n",
    "            print(f\"\\nAntigen: {antigen}\")\n",
    "            print(f\"Generated Antibody: {generated_antibody}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Generation failed for antigen {antigen}: {e}\")\n",
    "            # Try simpler generation\n",
    "            try:\n",
    "                outputs = model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    max_new_tokens=20,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                print(f\"Simple generation: {generated_text}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Even simple generation failed: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19dfc907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU CHECK ===\n",
      "CUDA available: True\n",
      "GPU count: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3090 Ti\n",
      "\n",
      "=== LOADING FINE-TUNED MODEL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 193.84it/s]\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully!\n",
      "\n",
      "=== TESTING MODEL UNDERSTANDING ===\n",
      "\n",
      "Test 1: Antigen: MKFLVNVALVFMVVYISYIYA\n",
      "Antibody: \n",
      "Top 10 most likely next tokens:\n",
      "   1. '\n",
      "' (score: 49.75)\n",
      "   2. 'Y' (score: 46.75)\n",
      "   3. 'D' (score: 46.25)\n",
      "   4. 'Q' (score: 46.25)\n",
      "   5. '1' (score: 46.00)\n",
      "   6. 'K' (score: 46.00)\n",
      "   7. 'V' (score: 46.00)\n",
      "   8. 'E' (score: 45.75)\n",
      "   9. 'H' (score: 45.75)\n",
      "  10. 'M' (score: 45.75)\n",
      "\n",
      "Test 2: Antigen: ACDEFGHIKLMNPQRSTVWY\n",
      "Antibody: \n",
      "Top 10 most likely next tokens:\n",
      "   1. '\n",
      "' (score: 46.75)\n",
      "   2. '1' (score: 44.50)\n",
      "   3. 'Y' (score: 44.25)\n",
      "   4. 'T' (score: 43.50)\n",
      "   5. 'M' (score: 43.50)\n",
      "   6. 'A' (score: 43.50)\n",
      "   7. 'F' (score: 43.50)\n",
      "   8. 'Q' (score: 43.50)\n",
      "   9. 'V' (score: 43.50)\n",
      "  10. 'N' (score: 43.25)\n",
      "\n",
      "=== TESTING AMINO ACID KNOWLEDGE ===\n",
      "Amino acid likelihood scores:\n",
      "  A: 40.50\n",
      "  T: 40.25\n",
      "  H: 40.00\n",
      "  K: 40.00\n",
      "  G: 39.75\n",
      "  C: 39.50\n",
      "  M: 39.50\n",
      "  Q: 39.50\n",
      "  D: 39.25\n",
      "  F: 39.25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Force 3090 Ti only\n",
    "\n",
    "# Restart Python kernel or run this in a fresh session\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Verify only 3090 Ti is visible\n",
    "print(\"=== GPU CHECK ===\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 0: {torch.cuda.get_device_name(0)}\")  # Should be 3090 Ti\n",
    "\n",
    "# Load your model (this should work since training worked)\n",
    "print(\"\\n=== LOADING FINE-TUNED MODEL ===\")\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./phi35-antibody-lora\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ").cuda()\n",
    "\n",
    "# Load fine-tuned adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./phi35-antibody-lora\")\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "\n",
    "# Simple test without the problematic generation\n",
    "print(\"\\n=== TESTING MODEL UNDERSTANDING ===\")\n",
    "\n",
    "# Test the model's learned patterns with forward pass only\n",
    "test_cases = [\n",
    "    \"Antigen: MKFLVNVALVFMVVYISYIYA\\nAntibody: \",\n",
    "    \"Antigen: ACDEFGHIKLMNPQRSTVWY\\nAntibody: \"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_cases):\n",
    "    print(f\"\\nTest {i+1}: {prompt}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # Forward pass to see what the model predicts\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[0, -1, :]  # Last token logits\n",
    "            \n",
    "            # Get top 10 most likely next tokens\n",
    "            top_k = torch.topk(logits, 10)\n",
    "            \n",
    "            print(\"Top 10 most likely next tokens:\")\n",
    "            for j, (score, token_id) in enumerate(zip(top_k.values, top_k.indices)):\n",
    "                token = tokenizer.decode(token_id.item())\n",
    "                print(f\"  {j+1:2d}. '{token}' (score: {score.item():.2f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Forward pass failed: {e}\")\n",
    "\n",
    "# Test amino acid understanding\n",
    "print(\"\\n=== TESTING AMINO ACID KNOWLEDGE ===\")\n",
    "amino_prompt = \"Antigen: A\\nAntibody: \"\n",
    "inputs = tokenizer(amino_prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Check if amino acids are highly ranked\n",
    "        amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "        amino_scores = []\n",
    "        \n",
    "        for aa in amino_acids:\n",
    "            aa_token_id = tokenizer.encode(aa, add_special_tokens=False)[0]\n",
    "            score = logits[aa_token_id].item()\n",
    "            amino_scores.append((aa, score))\n",
    "        \n",
    "        # Sort by score\n",
    "        amino_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"Amino acid likelihood scores:\")\n",
    "        for aa, score in amino_scores[:10]:\n",
    "            print(f\"  {aa}: {score:.2f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Amino acid test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2317216a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CHECKING TRAINING RESULTS ===\n",
      "LoRA adapter weights (should not be zeros):\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: mean=0.000028, std=0.011267\n",
      "Model file adapter_model.safetensors: 3.01 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CHECKING TRAINING RESULTS ===\")\n",
    "\n",
    "# Compare a PEFT model parameter before/after\n",
    "print(\"LoRA adapter weights (should not be zeros):\")\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora_A' in name or 'lora_B' in name:\n",
    "        print(f\"{name}: mean={param.data.mean().item():.6f}, std={param.data.std().item():.6f}\")\n",
    "        break  # Just show one example\n",
    "\n",
    "# Check if model files exist and have reasonable sizes\n",
    "import os\n",
    "model_dir = \"./phi35-antibody-lora\"\n",
    "if os.path.exists(model_dir):\n",
    "    for file in os.listdir(model_dir):\n",
    "        if file.endswith('.bin') or file.endswith('.safetensors'):\n",
    "            size = os.path.getsize(os.path.join(model_dir, file)) / (1024*1024)  # MB\n",
    "            print(f\"Model file {file}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e909df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUCCESSFUL TRAINING CONFIRMED ===\n",
      "✓ LoRA weights are trained (non-zero)\n",
      "✓ Adapter file exists (3.01 MB)\n",
      "✓ Your fine-tuning worked!\n",
      "\n",
      "=== TESTING YOUR TRAINED MODEL ===\n",
      "Prompt: 'Antigen: MKFLVNVALVFMVVYISYIYA\n",
      "Antibody: '\n",
      "Top predicted next tokens:\n",
      "  Amino acid/relevant tokens:\n",
      "    'Y': 46.75\n",
      "    'Q': 46.25\n",
      "    'D': 46.25\n",
      "    'V': 46.00\n",
      "    'K': 46.00\n",
      "    'H': 45.75\n",
      "    'E': 45.75\n",
      "    'M': 45.75\n",
      "  Other tokens:\n",
      "    '': 49.75\n",
      "    '1': 46.00\n",
      "\n",
      "Prompt: 'Antigen: ACDEFGHIKLMNPQRSTVWY\n",
      "Antibody: H'\n",
      "Top predicted next tokens:\n",
      "  Amino acid/relevant tokens:\n",
      "    'HH': 47.00\n",
      "    'Y': 47.00\n",
      "    'G': 46.75\n",
      "    'V': 46.75\n",
      "    'I': 46.50\n",
      "    'L': 46.25\n",
      "    'Q': 46.25\n",
      "    'K': 46.25\n",
      "  Other tokens:\n",
      "    'J': 45.50\n",
      "\n",
      "Prompt: 'Antigen: TEST\n",
      "Antibody: HEAVY'\n",
      "Top predicted next tokens:\n",
      "  Amino acid/relevant tokens:\n",
      "    'CH': 53.50\n",
      "    'C': 52.00\n",
      "    'AND': 51.75\n",
      "    'M': 51.75\n",
      "    'T': 51.25\n",
      "  Other tokens:\n",
      "    '': 55.50\n",
      "    '_': 53.50\n",
      "    '-': 53.25\n",
      "\n",
      "=== COMPARING BASE vs FINE-TUNED BEHAVIOR ===\n",
      "Base model predictions (LoRA disabled):\n",
      "Prompt: 'Antigen: MKFL\n",
      "Antibody: '\n",
      "Top predicted next tokens:\n",
      "  Amino acid/relevant tokens:\n",
      "  Other tokens:\n",
      "    '1': 43.75\n",
      "    '2': 42.75\n",
      "    '9': 42.50\n",
      "\n",
      "Fine-tuned model predictions (LoRA enabled):\n",
      "Prompt: 'Antigen: MKFL\n",
      "Antibody: '\n",
      "Top predicted next tokens:\n",
      "  Amino acid/relevant tokens:\n",
      "    'H': 41.50\n",
      "  Other tokens:\n",
      "    '1': 46.75\n",
      "    '2': 46.00\n",
      "    '3': 45.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== SUCCESSFUL TRAINING CONFIRMED ===\")\n",
    "print(\"✓ LoRA weights are trained (non-zero)\")\n",
    "print(\"✓ Adapter file exists (3.01 MB)\")\n",
    "print(\"✓ Your fine-tuning worked!\")\n",
    "\n",
    "print(\"\\n=== TESTING YOUR TRAINED MODEL ===\")\n",
    "\n",
    "# Test 1: Compare model behavior on your training format\n",
    "def test_model_predictions(prompt, model, tokenizer):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]  # Last token predictions\n",
    "        \n",
    "        # Get top predictions\n",
    "        top_k = torch.topk(logits, 15)\n",
    "        \n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print(\"Top predicted next tokens:\")\n",
    "        \n",
    "        amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY|\")\n",
    "        amino_predictions = []\n",
    "        other_predictions = []\n",
    "        \n",
    "        for score, token_id in zip(top_k.values, top_k.indices):\n",
    "            token = tokenizer.decode(token_id.item()).strip()\n",
    "            if any(c in amino_acids for c in token):\n",
    "                amino_predictions.append((token, score.item()))\n",
    "            else:\n",
    "                other_predictions.append((token, score.item()))\n",
    "        \n",
    "        print(\"  Amino acid/relevant tokens:\")\n",
    "        for token, score in amino_predictions[:8]:\n",
    "            print(f\"    '{token}': {score:.2f}\")\n",
    "        \n",
    "        print(\"  Other tokens:\")\n",
    "        for token, score in other_predictions[:3]:\n",
    "            print(f\"    '{token}': {score:.2f}\")\n",
    "        print()\n",
    "\n",
    "# Test different antibody prompts\n",
    "test_prompts = [\n",
    "    \"Antigen: MKFLVNVALVFMVVYISYIYA\\nAntibody: \",\n",
    "    \"Antigen: ACDEFGHIKLMNPQRSTVWY\\nAntibody: H\",\n",
    "    \"Antigen: TEST\\nAntibody: HEAVY\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    try:\n",
    "        test_model_predictions(prompt, model, tokenizer)\n",
    "    except Exception as e:\n",
    "        print(f\"Test failed for '{prompt}': {e}\")\n",
    "\n",
    "# Test 2: Compare with and without LoRA\n",
    "print(\"=== COMPARING BASE vs FINE-TUNED BEHAVIOR ===\")\n",
    "\n",
    "# Disable LoRA temporarily to see base model behavior\n",
    "model.disable_adapter_layers()\n",
    "print(\"Base model predictions (LoRA disabled):\")\n",
    "test_model_predictions(\"Antigen: MKFL\\nAntibody: \", model, tokenizer)\n",
    "\n",
    "# Re-enable LoRA to see fine-tuned behavior  \n",
    "model.enable_adapter_layers()\n",
    "print(\"Fine-tuned model predictions (LoRA enabled):\")\n",
    "test_model_predictions(\"Antigen: MKFL\\nAntibody: \", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e1b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYZING WHAT YOUR MODEL LEARNED ===\n",
      "\n",
      "Test: Should predict amino acid sequences\n",
      "Prompt: 'Antigen: '\n",
      "  Top amino acids: M(0.001), A(0.001), H(0.001), C(0.000), N(0.000)\n",
      "  '|' separator probability: 0.000\n",
      "\n",
      "Test: Should predict Heavy|Light format\n",
      "Prompt: 'Antibody: '\n",
      "  Top amino acids: A(0.001), H(0.000), C(0.000), D(0.000), E(0.000)\n",
      "  '|' separator probability: 0.001\n",
      "\n",
      "Test: Should start antibody sequence\n",
      "Prompt: 'Antigen: ABC\n",
      "Antibody: '\n",
      "  Top amino acids: D(0.001), Y(0.001), A(0.000), V(0.000), I(0.000)\n",
      "  '|' separator probability: 0.000\n",
      "\n",
      "Test: Should continue with light chain\n",
      "Prompt: 'Antigen: DEF\n",
      "Antibody: HEAVY|'\n",
      "  Top amino acids: L(0.005), H(0.001), M(0.001), A(0.001), S(0.001)\n",
      "  '|' separator probability: 0.000\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ANALYZING WHAT YOUR MODEL LEARNED ===\")\n",
    "\n",
    "# Test the model's understanding of your training format\n",
    "format_tests = [\n",
    "    (\"Antigen: \", \"Should predict amino acid sequences\"),\n",
    "    (\"Antibody: \", \"Should predict Heavy|Light format\"),\n",
    "    (\"Antigen: ABC\\nAntibody: \", \"Should start antibody sequence\"),\n",
    "    (\"Antigen: DEF\\nAntibody: HEAVY|\", \"Should continue with light chain\"),\n",
    "]\n",
    "\n",
    "for prompt, description in format_tests:\n",
    "    print(f\"\\nTest: {description}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            # Check specific token probabilities\n",
    "            amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "            separator_token = \"|\"\n",
    "            \n",
    "            # Get probabilities for amino acids\n",
    "            amino_probs = []\n",
    "            for aa in amino_acids:\n",
    "                try:\n",
    "                    aa_id = tokenizer.encode(aa, add_special_tokens=False)[0]\n",
    "                    prob = torch.softmax(logits, dim=-1)[aa_id].item()\n",
    "                    amino_probs.append((aa, prob))\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Get probability for separator\n",
    "            try:\n",
    "                sep_id = tokenizer.encode(separator_token, add_special_tokens=False)[0]\n",
    "                sep_prob = torch.softmax(logits, dim=-1)[sep_id].item()\n",
    "            except:\n",
    "                sep_prob = 0.0\n",
    "            \n",
    "            # Sort amino acids by probability\n",
    "            amino_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"  Top amino acids: {', '.join([f'{aa}({p:.3f})' for aa, p in amino_probs[:5]])}\")\n",
    "            print(f\"  '|' separator probability: {sep_prob:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
