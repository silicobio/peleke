{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ab9839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/GitHub/peleke/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared!\n",
      "Tokenizer vocab size: 100357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:11<00:00,  1.88s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model vocab size after resize: 100357\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"GPU memory cleared!\")\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "model_name = \"microsoft/phi-4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# ADD ALL THE SPECIAL TOKENS IN THE SAME ORDER AS TRAINING\n",
    "# 1. Epitope tokens\n",
    "epitope_tokens = [\"<epi>\", \"</epi>\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": epitope_tokens})\n",
    "\n",
    "# 2. Amino acids and other tokens\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "extra_tokens = amino_acids + [\"|\"]\n",
    "new_tokens = [t for t in extra_tokens if t not in tokenizer.get_vocab()]\n",
    "if new_tokens:\n",
    "    tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# 3. Task-specific tokens (this was missing!)\n",
    "task_tokens = [\"Antigen\", \"Antibody\", \"Epitope\"]\n",
    "tokenizer.add_tokens(task_tokens)\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Load base model with your device map\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map={'model.embed_tokens': 0,\n",
    "                'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0,\n",
    "                'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0,\n",
    "                'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0,\n",
    "                'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0,\n",
    "                'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0,\n",
    "                'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0,\n",
    "                'model.layers.24': 0, 'model.layers.25': 0,\n",
    "                'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1,\n",
    "                'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1,\n",
    "                'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1,\n",
    "                'model.layers.38': 1, 'model.layers.39': 1,\n",
    "                'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1},\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Resize model embeddings to match the tokenizer\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Now the vocab size should match - check it\n",
    "print(f\"Model vocab size after resize: {base_model.get_input_embeddings().weight.shape[0]}\")\n",
    "\n",
    "# Load your trained LoRA adapters\n",
    "model_path = \"/home/nicholas/Documents/GitHub/peleke/models/peleke-phi-4-0806025\"\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b642b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompt 1:\n",
      "Antigen: KVFGRCELAAAM[K][R]HGL[D][N][Y]RG[Y][S]LG[N]WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA[K]KIVSDGNGMNAWVAWRNRCK[G][T][D]V[Q]AW[I][R]GCRL<|im_end|>\n",
      "Antibody:\n",
      "--------------------------------------------------\n",
      "Test prompt 2:\n",
      "Antigen: NLCPFHEVFNATTFASVYAWNRKRISNCVADYSVIYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVI[R]G[N]EV[S][Q]IAPGQ[T]GNIADYNYKLPDDFTGCVIAWNSN[K]LDSKPSGNYNYLYRLLRKSKLKPFERDISTEIYQAGNKPCNGVAGPNCYSPLQSYGF[R]P[T][Y][G][V]GH[Q]PYRVVVLSFELLHAPATVCGP<|im_end|>\n",
      "Antibody:\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to create test prompts (same format as training)\n",
    "def create_test_prompt(antigen_with_epitopes):\n",
    "    return f\"Antigen: {antigen_with_epitopes}<|im_end|>\\nAntibody:\"\n",
    "\n",
    "# Test with some examples from your training data\n",
    "test_antigens = [\n",
    "    \"KVFGRCELAAAM[K][R]HGL[D][N][Y]RG[Y][S]LG[N]WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA[K]KIVSDGNGMNAWVAWRNRCK[G][T][D]V[Q]AW[I][R]GCRL\",\n",
    "    \"NLCPFHEVFNATTFASVYAWNRKRISNCVADYSVIYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVI[R]G[N]EV[S][Q]IAPGQ[T]GNIADYNYKLPDDFTGCVIAWNSN[K]LDSKPSGNYNYLYRLLRKSKLKPFERDISTEIYQAGNKPCNGVAGPNCYSPLQSYGF[R]P[T][Y][G][V]GH[Q]PYRVVVLSFELLHAPATVCGP\",\n",
    "]\n",
    "\n",
    "for i, antigen in enumerate(test_antigens):\n",
    "    prompt = create_test_prompt(antigen)\n",
    "    print(f\"Test prompt {i+1}:\")\n",
    "    print(prompt)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa18f68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Antibody Sequence:\n",
      "QLVQSGAEVKKPGSSVKVSCTASGFNIKDYYAVSWVRQAPGQGLEWMGWISYNGDTNYAQRFQGRVTITADKSTRTAYMELTSDDSAVYFCARERGDGYFAVWGQGTLVTVSS|DIQLTQSPDSLAVSLGERATINCKSSQNNKNYLAWYQQKPGQPPKLLIFATSKLESGVPVRFSGSGSGTDFTLNIHPVEEEDAATYYCQQANSFPYTFGGGTKLEIK\n",
      "\n",
      "Full Generation:\n",
      "Antigen: KVFGRCELAAAM<epi>K</epi><epi>R</epi>HGL<epi>D</epi><epi>N</epi><epi>Y</epi>RG<epi>Y</epi><epi>S</epi>LG<epi>N</epi>WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA<epi>K</epi>KIVSDGNGMNAWVAWRNRCK<epi>G</epi><epi>T</epi><epi>D</epi>V<epi>Q</epi>AW<epi>I</epi><epi>R</epi>GCRL<|im_end|>Antibody: QLVQSGAEVKKPGSSVKVSCTASGFNIKDYYAVSWVRQAPGQGLEWMGWISYNGDTNYAQRFQGRVTITADKSTRTAYMELTSDDSAVYFCARERGDGYFAVWGQGTLVTVSS|DIQLTQSPDSLAVSLGERATINCKSSQNNKNYLAWYQQKPGQPPKLLIFATSKLESGVPVRFSGSGSGTDFTLNIHPVEEEDAATYYCQQANSFPYTFGGGTKLEIK<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "def generate_antibody(model, tokenizer, antigen_with_epitopes, max_length=1000, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"Generate antibody sequence from epitope-highlighted antigen\"\"\"\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = create_test_prompt(antigen_with_epitopes)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,  # Max tokens to generate\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated sequence\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Extract just the antibody part\n",
    "    if \"Antibody:\" in generated_text:\n",
    "        antibody_part = generated_text.split(\"Antibody:\", 1)[1]\n",
    "        if \"<|im_end|>\" in antibody_part:\n",
    "            antibody_sequence = antibody_part.split(\"<|im_end|>\", 1)[0].strip()\n",
    "        else:\n",
    "            antibody_sequence = antibody_part.strip()\n",
    "    else:\n",
    "        antibody_sequence = \"Generation failed\"\n",
    "    \n",
    "    return antibody_sequence, generated_text\n",
    "\n",
    "# Test generation\n",
    "test_antigen = \"KVFGRCELAAAM<epi>K</epi><epi>R</epi>HGL<epi>D</epi><epi>N</epi><epi>Y</epi>RG<epi>Y</epi><epi>S</epi>LG<epi>N</epi>WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA<epi>K</epi>KIVSDGNGMNAWVAWRNRCK<epi>G</epi><epi>T</epi><epi>D</epi>V<epi>Q</epi>AW<epi>I</epi><epi>R</epi>GCRL\"\n",
    "\n",
    "antibody_seq, full_generation = generate_antibody(model, tokenizer, test_antigen)\n",
    "\n",
    "print(\"Generated Antibody Sequence:\")\n",
    "print(antibody_seq)\n",
    "print(\"\\nFull Generation:\")\n",
    "print(full_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfd40b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: KVFGRCELAAAM[K][R]HGL[D][N][Y]RG[Y][S]LG[N]WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA[K]KIVSDGNGMNAWVAWRNRCK[G][T][D]V[Q]AW[I][R]GCRL\n",
      "Converted: KVFGRCELAAAM<epi>K</epi><epi>R</epi>HGL<epi>D</epi><epi>N</epi><epi>Y</epi>RG<epi>Y</epi><epi>S</epi>LG<epi>N</epi>WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA<epi>K</epi>KIVSDGNGMNAWVAWRNRCK<epi>G</epi><epi>T</epi><epi>D</epi>V<epi>Q</epi>AW<epi>I</epi><epi>R</epi>GCRL\n",
      "----------------------------------------\n",
      "Original: NLCPFHEVFNATTFASVYAWNRKRISNCVADYSVIYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVI[R]G[N]EV[S][Q]IAPGQ[T]GNIADYNYKLPDDFTGCVIAWNSN[K]LDSKPSGNYNYLYRLLRKSKLKPFERDISTEIYQAGNKPCNGVAGPNCYSPLQSYGF[R]P[T][Y][G][V]GH[Q]PYRVVVLSFELLHAPATVCGP\n",
      "Converted: NLCPFHEVFNATTFASVYAWNRKRISNCVADYSVIYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVI<epi>R</epi>G<epi>N</epi>EV<epi>S</epi><epi>Q</epi>IAPGQ<epi>T</epi>GNIADYNYKLPDDFTGCVIAWNSN<epi>K</epi>LDSKPSGNYNYLYRLLRKSKLKPFERDISTEIYQAGNKPCNGVAGPNCYSPLQSYGF<epi>R</epi>P<epi>T</epi><epi>Y</epi><epi>G</epi><epi>V</epi>GH<epi>Q</epi>PYRVVVLSFELLHAPATVCGP\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_brackets_to_epi(sequence):\n",
    "    \"\"\"Convert [X] format to <epi>X</epi> format\"\"\"\n",
    "    return re.sub(r'\\[([A-Z])\\]', r'<epi>\\1</epi>', sequence)\n",
    "\n",
    "# Test it\n",
    "# test_sequence = \"SCNGLYYQGSCYI[L]HSD[Y]KSFEDAK[D][Y]V[E][D][T]\"\n",
    "# converted = convert_brackets_to_epi(test_sequence)\n",
    "# print(\"Original:\", test_sequence)\n",
    "# print(\"Converted:\", converted)\n",
    "\n",
    "# Convert multiple sequences\n",
    "sequences_with_brackets = [\n",
    "    \"KVFGRCELAAAM[K][R]HGL[D][N][Y]RG[Y][S]LG[N]WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA[K]KIVSDGNGMNAWVAWRNRCK[G][T][D]V[Q]AW[I][R]GCRL\",\n",
    "    \"NLCPFHEVFNATTFASVYAWNRKRISNCVADYSVIYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVI[R]G[N]EV[S][Q]IAPGQ[T]GNIADYNYKLPDDFTGCVIAWNSN[K]LDSKPSGNYNYLYRLLRKSKLKPFERDISTEIYQAGNKPCNGVAGPNCYSPLQSYGF[R]P[T][Y][G][V]GH[Q]PYRVVVLSFELLHAPATVCGP\",\n",
    "]\n",
    "\n",
    "converted_sequences = [convert_brackets_to_epi(seq) for seq in sequences_with_brackets]\n",
    "\n",
    "for orig, conv in zip(sequences_with_brackets, converted_sequences):\n",
    "    print(f\"Original: {orig}\")\n",
    "    print(f\"Converted: {conv}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd72d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 1 ===\n",
      "Input antigen: KVFGRCELAAAM<epi>K</epi><epi>R</epi>HGL<epi>D</epi><epi>N</epi><epi>Y</epi>RG<epi>Y</epi><epi>S</epi>LG<epi>N</epi>WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA<epi>K</epi>KIVSDGNGMNAWVAWRNRCK<epi>G</epi><epi>T</epi><epi>D</epi>V<epi>Q</epi>AW<epi>I</epi><epi>R</epi>GCRL\n",
      "Generated antibody: EVQLVESGGGLVKPGGSLKLSCAASGFTFSNYAMSWVRQTPEKRLEWVASISAGGSYTYYADSVKGRFTISRDNARNILYLQMNSLKTEDTAIYYCTRGELTYDHWGQGTLVTVSS|DIVMTQSPLSLPVTPGEPASISCRSSQSLLHRSGHTYLHWYLQRPGQSPQVLIIFGDNNRFSGVPDRFSGSGSGTDFTLKISRVEAEDVGVYYCMQGTHWPRTFGQGTKVEIK\n",
      "\n",
      "=== Test 2 ===\n",
      "Input antigen: NLCPFHEVFNATTFASVYAWNRKRISNCVADYSVIYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVI<epi>R</epi>G<epi>N</epi>EV<epi>S</epi><epi>Q</epi>IAPGQ<epi>T</epi>GNIADYNYKLPDDFTGCVIAWNSN<epi>K</epi>LDSKPSGNYNYLYRLLRKSKLKPFERDISTEIYQAGNKPCNGVAGPNCYSPLQSYGF<epi>R</epi>P<epi>T</epi><epi>Y</epi><epi>G</epi><epi>V</epi>GH<epi>Q</epi>PYRVVVLSFELLHAPATVCGP\n",
      "Generated antibody: EVQLVESGGGLIQPGGSLRLSCAASAFTVSSNYMSWVRQAPGKGLEWVSVIYPGGSTFYADSVKGRFTISRDNSKNTLYLQMRAEDTAVYYCAKVYGDSLDPWGQGTLVTV|ALTQPASVSGSPGQSITISCTGTSGGYNYVSWYQQHPGKAPKLMIYDVSNRPSGVSNRFSGSKSGNTASLTISGLQAEDEADYYCSSYTSSSTLVFGGGTKLTVG\n"
     ]
    }
   ],
   "source": [
    "# Generate multiple examples with different parameters\n",
    "test_antigens = [\n",
    "    \"KVFGRCELAAAM<epi>K</epi><epi>R</epi>HGL<epi>D</epi><epi>N</epi><epi>Y</epi>RG<epi>Y</epi><epi>S</epi>LG<epi>N</epi>WVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCA<epi>K</epi>KIVSDGNGMNAWVAWRNRCK<epi>G</epi><epi>T</epi><epi>D</epi>V<epi>Q</epi>AW<epi>I</epi><epi>R</epi>GCRL\",\n",
    "    \"NLCPFHEVFNATTFASVYAWNRKRISNCVADYSVIYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVI<epi>R</epi>G<epi>N</epi>EV<epi>S</epi><epi>Q</epi>IAPGQ<epi>T</epi>GNIADYNYKLPDDFTGCVIAWNSN<epi>K</epi>LDSKPSGNYNYLYRLLRKSKLKPFERDISTEIYQAGNKPCNGVAGPNCYSPLQSYGF<epi>R</epi>P<epi>T</epi><epi>Y</epi><epi>G</epi><epi>V</epi>GH<epi>Q</epi>PYRVVVLSFELLHAPATVCGP\",\n",
    "]\n",
    "\n",
    "for i, antigen in enumerate(test_antigens):\n",
    "    print(f\"\\n=== Test {i+1} ===\")\n",
    "    print(f\"Input antigen: {antigen}\")\n",
    "    \n",
    "    antibody_seq, _ = generate_antibody(model, tokenizer, antigen, temperature=0.7)\n",
    "    print(f\"Generated antibody: {antibody_seq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
